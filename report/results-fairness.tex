\subsubsection{Downlink fairness}
The setup for this test is the same as explained in the setup 3 of "Testing
Setups". Here the goal of the experiment is to analyse MPTCP's behaviour with
regards to interference, throughput and fairness to other TCP flows on the
downlink. For this experiment 2 AP's are used; one on 5GHz frequency band and
the other on 2.4GHz. To ensure that the AP's have the same amount of load, each
parallel machine connects to one AP while the MPTCP machine connects to both AP
using one of its interface for each; thereby making it seem as if each AP is
connected to 2 clients. This also applies to the two servers to which the TCP
connections would be made. Netperf is used to create downlink traffic between
the servers and client; this lasts for two minutes time slots. This is ran
multiple times and the aggregate result can be seen in figure FIG.             % TODO: correct fig (maybe graphs from 14-08-2013).

The coupled congestion control algorithm is used by all machines in this test.
For the parallel machines this is has no consequence since they only have one
TCP flow each so in essence it acts like the normal TCP congestion algorithm.
However for the MPTCP flows, coupled congestion control limits the aggregate
throughput to be as good as the throughput of the best link on that path. In
order words, the sum of the throughput should be the same as that of the single
5GHz channel. We do actually see this phenomenon from the graphs where the sum
of the MPTCP throughputs is about the same as that of the best parallel link
save for some slight variation which can be attributed to the wireless
environment in which these experiments where carried out.

From the graph we see MPTCP also tries to achieve a form of load balancing by
moving data off the congested path in this case the 2.5GHz link onto the 5GHz
link.

It should be noted that this also shows that MPTCP running the coupled
congestion control algorithm ensures fairness on the downlink. It does this with
the aid of loss feedback from the channel; so as packets build up in the AP, the
queue fills up leading to packet drops and then loss. This indicates to the
MPTCP sender to slow down the rate.

\subsubsection{Uplink fairness}
As previously mentioned most of the final experiments were run simultaneously
between 3 PCs (2 running parallel and 1 MPTCP) and the results of these were
quite interesting. Most of the graphs showed MPTCP performing as well as
parallel (choose appropriate graphs) but they also showed that MPTCP was not
being fair as it was using up more than its fair amount of share at the cost of
other clients, in other words it violated rule number 2 of the coupled
congestion control (do not harm). After seeing similar results for several tests
it was noted that the congestion window was growing quite aggressively which led
to much debate and investigation. Afterwards it was found that the reason the
cwnd was growing so aggressively was that the sender was not seeing any loss
(hence the agressive growth). This was caused by the local queue at the sender,
as it was later found, the IP queue is very large (128KB) so it keeps receiving
packets and keeps the queue full at all times, never overflowing and never
causing loss; even when packets are sent to the socket buffer the ip queue
simply takes more packets to replace those it sent. This is not ideal for MPTCP,
the queue is always full and sending, the cwnd keeps growing and the alpha
variable cannot cap the growth because it sees no loss, so in conclusion MPTCP
does not work correctly (the cwnd size is not correct).

Different solutions to this problem have been considered, the most relevant ones
being: reducing the number of retries, TSQ, calculating the RTT variance every
ack (instead of every RTT) and Master socket/MPTCP socket)
