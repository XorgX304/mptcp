
In the previous section it was shown that multiple interfaces are better than a
single one, but what exactly does MPTCP do with multiple interfaces? The
question that should be asked now is how does MPTCP work with multiple
interfaces and how much better does it behave when compared against parallel
experiments.

As stated in the previous section a lot of experiments were run in order to
observe how MPTCP behaves, it was decided that the results from the MPTCP
experiments should be compared against the parallel ones as they are the
benchmark of how well the network can behave. To make the results more
conclusive and accurate different congestion control mechanisms were used (reno
and coupled).

Different sets of experiments were run in a span of several days using the same
setup. During the first few days all the experiments were run during 10 minutes
using the coupled congestion-control, 5 minutes for the upstream and 5 minutes
for the downstream. However, after observing some unusual results between the up
and down tests, where the down tests would in general be fairer at distributing
the load because they could see loss and correctly adjust the cwnd, it was
decided that the lenght of the experiments would be increased to 15 minutes
each. Later on experiments using the reno congestion-control were also run in
order to compare results against experiments using coupled. After a couple of
weeks running this setup, the lenght of the experiments was changed to 2 minutes
and 3 different machines would run in simultaneous using both reno and coupled.

\subsubsection{Upstream and Downstream}

\begin{figure}[h]
 \centering
 \subfloat[][downlink] {\
   \scalebox{0.55}{\input{graphs/down-fair.tex}}\label{graph:down-fair}
 }
 \subfloat[][uplink] {\
   \scalebox{0.55}{\input{graphs/up-fair.tex}}\label{graph:up-fair}
 }

 \caption{Traffic distribution on downlink vs.\ uplink}\label{graph:interference}
\end{figure}

In the start both upstream and downstream experiments were run, but after
several rounds it became clear that the downstream experiments were very similar
and regular showing a fair distribution (throughput and utilization) between
both networks as can be seen in figure~\ref{graph:down-fair} (which did not
prove the same for upstream show in figure~\ref{graph:up-fair}). In spite of
that it was decided at the time that upstream only experiments were to be run
more extensively. It was later discovered that due to the sender not seeing
loss, the cwnd would grow aggressively and the tests behave in such an unfair
way (as will be explained later in the report). When the setup was changed to
run the simultaneous experiments more rounds of downstream experiments were run
to support our findings.

% Note that towards the end we were merging results across multiple tests and
% looking at the CDFs for these aggregated tests

\subsubsection{2.4 cross-channel interference}
Tests between cross-channel (different channels) 2.4 networks showed that there
is quite a lot of interference between them even though they are in different
channels (usually 1/5 or 1/11). This was quite surprising as it was expected
that by having each network in different channels interference would be kept to
a minimum. There could be different reasons for this such as the APs not having
strong enough antennas (more likely to be susceptible to interference) or the
fact the APs might be positioned very close to each other which could cause
self-interference.

% TODO: Correct figure refernces here
As can be seen in \texttt{02-08-13/mptcp@1a-up} and
\texttt{01-08-13/mptcp@11-up} (i/ni mptcp experiments) the throughput in
\texttt{mptcp@1a-up} is close to the one in \texttt{01-08-13/mptcp@11-up}, but
significantly worse in one of the links; however the other link is very similar
which is quite surprising. The throughputs average around 20 Mbps for for ni and
15 for i. So even if so slightly the ni experiments are performing quite close
to the interfering ones which should not happen in theory. Experiments with the
APs positioned far apart showed a much fairer distribution between the 2
networks as usually the network in the less busy channel dominates the
throughput.

The comparison between the ni mptcp against the parallel graph, showed that
MPTCP is underperforming (?) as the aggregate throughput of the MPTCP flows does
not amount to the best single path; this can be seen in the cdf graph
\texttt{01-08-13/coupled-1} which uses the coupled congestion control.

For reno, the results (as expected) are slightly better, but the graph should
show all the lines on top of each other which is not the case as can be seen in
\texttt{01-08-13/reno-1} (even though the aggregate throughput of MPTCP is
roughly the same as the best single path).

\subsubsection{2.4 same-channel interference}
% TODO: Correct figure refernces here
As expected tests using the same channel showed a lot of interference, usually
with one of the networks being favoured over the other (one network would have a
quite high throughput around 15 Mbps while the other would have a very low one
around 6 Mbps). This could be due to the fact that the coupled congestion
control favours the less lossy link so it sends more traffic through that link
and barely any through to the other (This could be seen when the graphs were
compared to the single tests, in which separately each network performed
similarly but when run simultaneously or in parallel one of the networks was
always favoured) as in \texttt{02-08-13/mptcp@1a-up}. When the
congestion-control was switched to Reno it was apparent right away that the
distribution between the links was much fairer and better distributed. This
happens because reno unlike coupled runs congestion control separately for each
interface so it will try to fully utilise both networks resulting in a fairer
distribution as can be seen in \texttt{02-08-13/mptcp@1b-up}. Coupled as
mentioned before avoids lossy links so it will avoid loading that link with
traffic and only using it as backup for when the better link is full.

Reno might be more appropriate for networks on different channels as it will not
avoid lossy links but try to equally share the load between the 2 channels
despite of how lossy the links are. This would be fairer to both networks as
both get to send without one being favoured over the other.

The CDF graph for MPTCP using coupled and parallel show that MPTCP is
outperforming the best single path which should not happen (as the aggregate
throughput should be the same as the best path not more - it's violating rule 2:
do no harm); this means that MPTCP is getting more than its fair share of
throughput.

The same CDF but running reno behaves as expected with most of the lines on top
of each other (a fair distribution) except one of the MPTCP lines which is doing
slighlty worse than the rest.

\subsubsection{Downlink fairness}
The setup for this test is the same as explained in the setup 3 of "Testing
Setups". Here the goal of the experiment is to analyse MPTCP's behaviour with
regards to interference, throughput and fairness to other TCP flows on the
downlink. For this experiment 2 AP's are used; one on 5GHz frequency band and
the other on 2.4GHz. To ensure that the AP's have the same amount of load, each
parallel machine connects to one AP while the MPTCP machine connects to both AP
using one of its interface for each; thereby making it seem as if each AP is
connected to 2 clients. This also applies to the two servers to which the TCP
connections would be made. Netperf is used to create downlink traffic between
the servers and client; this lasts for two minutes time slots. This is ran
multiple times and the aggregate result can be seen in figure FIG.             % TODO: correct fig (maybe graphs from 14-08-2013).

The coupled congestion control algorithm is used by all machines in this test.
For the parallel machines this is has no consequence since they only have one
TCP flow each so in essence it acts like the normal TCP congestion algorithm.
However for the MPTCP flows, coupled congestion control limits the aggregate
throughput to be as good as the throughput of the best link on that path. In
order words, the sum of the throughput should be the same as that of the single
5GHz channel. We do actually see this phenomenon from the graphs where the sum
of the MPTCP throughputs is about the same as that of the best parallel link
save for some slight variation which can be attributed to the wireless
environment in which these experiments where carried out.

From the graph we see MPTCP also tries to achieve a form of load balancing by
moving data off the congested path in this case the 2.5GHz link onto the 5GHz
link.

It should be noted that this also shows that MPTCP running the coupled
congestion control algorithm ensures fairness on the downlink. It does this with
the aid of loss feedback from the channel; so as packets build up in the AP, the
queue fills up leading to packet drops and then loss. This indicates to the
MPTCP sender to slow down the rate.

\subsubsection{Uplink fairness}
As previously mentioned most of the final experiments were run simultaneously
between 3 PCs (2 running parallel and 1 MPTCP) and the results of these were
quite interesting. Most of the graphs showed MPTCP performing as well as
parallel (choose appropriate graphs) but they also showed that MPTCP was not
being fair as it was using up more than its fair amount of share at the cost of
other clients, in other words it violated rule number 2 of the coupled
congestion control (do not harm). After seeing similar results for several tests
it was noted that the congestion window was growing quite aggressively which led
to much debate and investigation. Afterwards it was found that the reason the
cwnd was growing so aggressively was that the sender was not seeing any loss
(hence the agressive growth). This was caused by the local queue at the sender,
as it was later found, the IP queue is very large (128KB) so it keeps receiving
packets and keeps the queue full at all times, never overflowing and never
causing loss; even when packets are sent to the socket buffer the ip queue
simply takes more packets to replace those it sent. This is not ideal for MPTCP,
the queue is always full and sending, the cwnd keeps growing and the alpha
variable cannot cap the growth because it sees no loss, so in conclusion MPTCP
does not work correctly (the cwnd size is not correct).

Different solutions to this problem have been considered, the most relevant ones
being: reducing the number of retries, TSQ, calculating the RTT variance every
ack (instead of every RTT) and Master socket/MPTCP socket)
