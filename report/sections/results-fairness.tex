In the previous section, we showed that Multipath TCP can benefit from
using multiple wireless links at the same time. As expected, we also saw that
Multipath TCP with New Reno was using an unfair share of the networks'
resources; approximately half of the total available bandwidth when there was a
competing flow on each network. One of the goals of Coupled congestion 
control is bottleneck fairness; more specifically, a 
Multipath TCP flow should not take more of the bottleneck capacity than any 
other flow. The following section evaluates to what extent Multipath TCP with 
Coupled congestion control achieves fairness on wireless networks.

As in the previous section, we will evaluate Multipath TCP's behaviour on the
downlink first, and then on the uplink.

\subsubsection{Downlink fairness}

\begin{figure}[h]
 \centering
 \input{graphs/down-fair.tex}
 \caption{Traffic distribution, downlink, non-overlapping 2.4 GHz}\label{graph:down-fair}
\end{figure}

On the downlink, the Coupled algorithm on clients with only a single
interface acts like regular TCP New Reno. Since they will only use one subflow
each, Coupled will not try to limit how aggressively it sends data. However, for
the client with multiple interfaces, the Coupled algorithm should limit the
aggregate throughput to be as good as the throughput of the best link
available, as explained in \S\ref{sec:bg:mptcp}.\@ Figure~\ref{graph:down-fair}
shows that Coupled is indeed limiting one subflow and choosing to transmit most
of the traffic on the other.  Additionally, although the sum is not identical to
the throughput of either parallel link, it is close enough that we can consider
Coupled to be fair with regard to the other flows, since the
client with multiple interfaces is only taking a third of the total network
capacity, as opposed to half in the case of New Reno.

% Don't show send queue?

In \S\ref{sec:results-mptcp-down} we saw that congestion feedback in the 
downlink case is provided to the sender by the AP dropping packets if they
come in faster than they can be sent out out on the wireless channel. The
Coupled algorithm uses this feedback to correctly reduce its aggressiveness and 
thus avoid using an unfair share of the overall capacity.

\begin{figure}[h]
 \centering
 \input{graphs/cb-fairness-down.tex}
 \caption{Cross-band downlink test with Coupled}\label{graph:cb-fairness-down}
\end{figure}

Similar results are seen when the two wireless links are operating on separate
bands, as seen in Figure~\ref{graph:cb-fairness-down}.

\subsubsection{Uplink fairness}

\begin{figure}[h]
 \centering
 \input{graphs/up-fair.tex}
 \caption{Traffic distribution, uplink, 5 and 2.4 GHz}\label{graph:up-fair}
\end{figure}

Initially, we expected Multipath TCP with Coupled congestion control to perform
similarly in both the uplink and downlink cases. However, we soon found
that this was not the case. Most of the time plots and CDF graphs showed 
Multipath TCP performing as well as parallel on \textbf{both} links as seen in
Figure~\ref{graph:up-fair}, showing that the client with multiple interfaces was
not behaving fairly. Instead, the aggregrate throughput of both its links were taking more of the capacity than any of the single-flow clients as seen with TCP New Reno in \S\ref{sec:results-mptcp}.

\begin{figure}[h]
 \centering
 \input{graphs/fairness-up-close.tex}
 \caption{Congestion window size for Multipath TCP uplink connection}\label{graph:fairness-up-close}
\end{figure}

After seeing this result in same-channel, non-overlapping and cross-band  
tests, we reasoned that the problem had to be related somehow to the 
behaviour of WiFi on the uplink in general. Figure~\ref{graph:fairness-up-close} 
shows the congestion window as a function of time for one of the subflows in one 
such test. The bandwidth delay product shows the approximate number of packets
we expect the interface to have in flight at any given point in time. This was
calculated by taking the RTT reported by \texttt{ping} and multiplying it by the
average throughput we were seeing on that interface. The measured RTT was taken
when the network was idle, so it was doubled for this calculation. This gives
$20\text{ms} \cdot 10\text{Mbits} \approx 25\text{kB}$. The congestion window of
the link is not expected to surpass this for any significant period of time, yet
the graph clearly shows the congestion window is consistently much greater than
this.

Since Coupled uses the congestion window size to determine how aggressive it
should be, an inflated congestion window will make it more aggressive than the 
environment would suggest, resulting in exactly the behaviour we see in
Figure~\ref{graph:up-fair}.

To understand what is going on here, some background knowledge about how TCP
works in Linux, and how the Linux kernel manages network queues is needed. Linux
maintains the IP send queue as a list of pointers to data in the socket buffer. 
This list is allowed to grow very large, and so in practice will never overflow.
When TCP tries to send a packet, it will first check that there is room in
the current congestion window, and only then will it pass the packet to the
send queue. The NIC then takes packets  from the send queue in FIFO order to
transmit them on the medium.

Ignoring the advertised receive window, the only two limiting factors to how
quickly a TCP flow
can send are therefore the congestion window and the link speed of the NIC.\@ If
TCP passes packets into the queue faster than the NIC can transmit them, the
send queue will simply grow, but not drop packets. TCP will consider the
packet as sent, but the packet will be kept in the IP send queue until
the NIC is ready to transmit it. The relevant effect of this is that TCP will
count the time a packet is queued locally in its estimate of the RTT.

This behaviour is not evident on all wireless networks. A TCP flow
will usually start experiencing loss when it overflows a
buffer at a bottleneck, and this is generally not the immediate link.
The flow will eventually limit the number of packets it sends based on the
speed of the bottleneck, and since the speed of the immediate link is not
the bottleneck, the NIC should be able to send at least as fast as TCP passes
packets into the queue.

A mismatch of assumptions happens when the immediate link \textbf{is} the 
bottleneck of the path. The NIC will take a packet out of the queue, use carrier 
sense and potentially retry some number of times before actually sending the 
packet. It will, however, very rarely actually drop a packet because it usually 
succeeds in sending the packet before reaching its retry limit. Since TCP does 
not see any loss, it will assume that the path is not congested, and so it will 
continue to grow its congestion window.  In fact, the loss that TCP does see 
will likely \textbf{not} be related to congestion, such as faulty links or devices.
This will cause it to back off its congestion window incorrectly.

% TODO: mention MAC layer fairness?
Since the immediate link is not the bottleneck of the path, TCP does not see any loss and the NIC always has packets to send, this means that the MAC layer will ensure fairness between clients and limit the throughput.
However, TCP will still manage to fill the send queue with packets that the NIC is unable to send. This becomes an issue, because any packets sent by other flows on the same host will now be queued behind the TCP ones, increasing the delay significantly and negatively impacting flows where latency is more important than throughput such as VoIP.\@ To mitigate this problem, TCP Small Queues has been implemented in the Linux kernel, effectively hard capping the amount of bytes a TCP flow can keep in the IP send queue at any given point in time. Although this will not solve the underlying problem, it will limit the amount of delay a misbehaving TCP flow can inflict on other flows.

For Multipath TCP with Coupled congestion control, however, this becomes a
substantial problem. As explained in \S\ref{sec:bg:mptcp}, Coupled tries to not be too 
aggressive on any one link by throttling the growth of the congestion window.
This works correctly if the congestion window directly impacts how often the NIC 
puts packets on the network, as throttling the growth of the congestion window 
will effectively lead to other clients on the network getting a larger share of 
the network capacity. However, when the congestion window is sufficiently large 
that the NIC always has packets to send, this throttling has no effect at all. 
The NIC is always sending, regardless of how quickly the congestion window grows 
beyond that point, which means that each \textbf{subflow} is only limited by the 
MAC layer fairness, which is fair \textbf{per interface}. Thus, Coupled will end 
up behaving exactly like New Reno, which is unfair when multiple subflows are 
used.

This problem is not related to the amount of interference between the different
wireless networks, but rather to the lack of feedback given on
wireless networks when the link capacity is reached. It is also only experienced
when the immediate link is congested, as any bottleneck further along the network
path is likely to have a queue which will overflow, drop a packet, and thus
signal to TCP that it should back off. In fact, this is precisely what we see in
the downlink case where the queue in the AP effectively conveys how congested
the wireless link is to the sender.

\begin{figure}[h]
 \centering
 \subfloat[][CDF] {\
   \input{graphs/fairness-rtt-up-cdf.tex}\label{graph:fairness-rtt-up-cdf}
 }
 \\
 \subfloat[][Multipath TCP time plot] {\
   \input{graphs/fairness-rtt-up-close.tex}\label{graph:fairness-rtt-up-close}
   % TODO: congestion window is not /10 here, throughput is *10
   % update oddities section calculation to match this
 }
 \caption{Uplink, non-overlapping 2.4 GHz, +100ms}\label{graph:fairness-rtt-up}
\end{figure}

\subsubsection{Uplink fairness on high-RTT links}

Figure~\ref{graph:fairness-rtt-up} shows a test in which the RTT was
artificially increased by 100ms, by adding delay at the IP queue level on the
test server. In this case, we would not expect the higher RTT to significantly
affect overall throughput, but rather to lower the rate of growth of the
congestion window during slow-start. The CDF, however, shows that throughput is
drastically reduced.

In particular, consider the time plot of one of the Multipath
TCP flows in Figure~\ref{graph:fairness-rtt-up-close}. The bandwidth delay product line shows
the product of the real RTT + 100ms and the throughput observed without an
inflated RTT.\@ Clearly, the congestion window is never approaching this value.
The same is the case for the other subflow and the two parallel flows. By
looking at the number of transmission failures reported by the NIC, we can see
that the amount of dropped packets is similar to the number observed in tests
without artificially high RTT.\@ This kind of constant loss causes both Coupled
and New Reno to continuously halve their congestion windows, and with the higher
RTT, the additive increase is too slow to bring the congestion window up to the
bandwidth delay product before the next packet loss, after which the congestion 
window is halved again. Since TCP is now putting packets into the send queue 
much slower than the NIC can transmit them, the throughput is limited by the 
congestion window. Since the window is too small, the throughput suffers.

Coupled also suffers noticeably more from this problem than New Reno. This is
because Coupled is trying to behave fairly to other flows, but is reacting to
the wrong inputs.  Coupled reduces its aggressiveness if it detects that there
are competing flows, and this is again deduced from the loss rate. Since the
loss rate is close to constant, Coupled believes that there are other competing
flows, and thus reduces its aggressiveness. This further limits the growth of
each subflow's congestion window, which exaggerates the problem of the
congestion window being too small, further reducing throughput.

% TODO: We may need a better explanation for the above paragraph.

% TODO: mention that this is the case for certain non-wireless networks as well?
% TODO: should we discuss possible solutions here or somewhere else?
% vim:textwidth=80:colorcolumn=80:spell:
