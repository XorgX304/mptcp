In the previous section, we showed that Multipath TCP can in fact benefit from
using multiple wireless links at the same time. As expected, we also saw that
Multipath TCP with New Reno was taking an unfair share of the networks'
resources; about half of the total available bandwidth when there was a
competing flow on each network. One of the goals of the Multipath TCP's Coupled
congestion control is to to behave fairly with competing flows; more
specifically, a Multipath TCP flow should not take more of the bottleneck
capacity than any other flow. The following section evaluates to what extent
Multipath TCP with Coupled congestion control ensures fairness on wireless
networks.

As in the previous section, we will evaluate Multipath TCP's behaviour on the
downlink first, and then move on to the uplink. Both the following sections will
refer to figure~\ref{graph:fairness}

\begin{figure}[h]
 \centering
 \subfloat[][downlink] {\
   % TODO: Maybe make this a 5-2.4 as well?
   \scalebox{0.55}{\input{graphs/down-fair.tex}}\label{graph:down-fair}
 }
 \subfloat[][uplink] {\
   % TODO: Can we do another 5 up without the steps?
   % Otherwise, we should explain why we see the steps and the diverging 2.4s below
   \scalebox{0.55}{\input{graphs/up-fair.tex}}\label{graph:up-fair}
 }

 \caption{Traffic distribution on downlink vs.\ uplink}\label{graph:fairness}
\end{figure}

\subsubsection{Downlink fairness}
In the downlink case, the Coupled algorithm has no consequences for parallel
clients with only a single interface, since they will only use one subflow. In
essence, the Multipath TCP flows behaves like regular TCP New Reno. However, for
the client with multiple interfaces, the Coupled algorithm should limit the
aggregate throughput to be as good as the throughput of the best link
available.\@ Figure~\ref{graph:down-fair} that Coupled is indeed limiting one
subflow and choosing to transmit most of the traffic on the other. Additionally,
although the sum is not identical to the throughput of either parallel link, it
is close enough that we can consider Coupled to be performing fairly with regard
to the other flows; that is, the client with multiple interfaces is only taking
a third of the total network capacity, as opposed to half in the case of New
Reno.

% Don't show send queue?

As pointed out in section~\ref{sec:results-mptcp-down}, congestion feedback in
the downlink case is provided to the sender by the AP dropping packets if they
come in faster than they can be sent out out on the wireless channel. The
Coupled algorithm uses this feedback to reduce its aggressiveness and thus avoid
using an unfair share of the overall capacity.

\begin{figure}[h]
 \centering
 \input{graphs/cb-fairness-down.tex}
 \caption{Cross-band downlink test with Coupled}\label{graph:cb-fairness-down}
\end{figure}

Similar results are seen when the two wireless links are operating on separate
bands, as seen in figure~\ref{graph:cb-fairness-down}. Coupled effectively uses
the 2.4 GHz link only when the 5 GHz network sees lowered throughput for periods
due to other clients using the network, which is precisely as expected. This
kind of adaptation to changing network characteristics is another goal of the
Coupled congestion control.

\subsubsection{Uplink fairness}
Initially, we expected Multipath TCP with Coupled congestion control to perform
similarly in both the uplink and downlink cases. However, we soon determined
that this was not the case. Most of the time plots and CDF graphs showed MPTCP
performing as well as parallel on \textbf{both} links as seen in
figure~\ref{graph:up-fair}, showing that the client with multiple interfaces was
not behaving fairly. Instead, it was taking more of the capacity than the other
single-flow clients.

\begin{figure}[h]
 \centering
 \input{graphs/fairness-up-close.tex}
 \caption{Congestion window size for Multipath TCP uplink connection}\label{graph:fairness-up-close}
\end{figure}

After seeing similar results for all channel and band combinations, we reasoned
that the problem had to be related somehow to the behaviour of WiFi on the
uplink in general. Figure~\ref{graph:fairness-up-close} shows the congestion
window as a function of time for one of the subflows in one such test. In
particular, the bandwidth delay product shows the approximate number of packets
we expect the interface to have in flight at any given point in time. This was
calculated by taking the RTT reported by \texttt{ping} and multiplying it by the
average throughput we were seeing on that interface. The measured RTT was taken
when the network was idle, so it was doubled for this calculation. This gives
$20\text{ms} \cdot 10\text{Mbits} \approx 25\text{kB}$. The congestion window of
the link is not expected to surpass this for any significant period of time, yet
the graph clearly shows the congestion window is consistently much greater than
this value.

Since Coupled uses the congestion window size to determine how aggressive it
should be, an inflated congestion window will clearly make it more aggressive
than the environment would suggest, resulting in the exact behaviour we see in
figure~\ref{graph:up-fair}.

To understand what is going on here, some background knowledge about how TCP
works in Linux, and how the Linux kernel manages network queues is needed. Linux
maintains the IP send queue as a list of pointers. This list is allowed to grow
very large, and so in practice will never overflow. When TCP tries to send a
packet, it will first check that there is room in the current congestion window,
and only then will it pass the packet to the send queue. As mentioned, the send
queue is generally able to receive packets. The NIC takes packets from the send
queue in FIFO order to transmit them on the medium.

Discounting the advertised receive window, the two limits to how fast a TCP flow
can send are therefore the congestion wndow and the link speed of the NIC.\@ If
TCP passes packets into the queue faster than the NIC can transmit them, the
send queue will simply grow, but not drop packets. The TCP flow counts the
packet as having been sent, but the packet will be kept in the send queue until
the NIC is ready to transmit it. The relevant effect of this is that TCP will
count the time a packet is queued locally in its estimate of the RTT.

This is not usually a problem, as a TCP flow will start seeing loss when it
overflows a buffer at a bottleneck; this is generally not the immediate link.
The flow will eventually converge to limiting the number of packets it sends to
the speed of the bottleneck, and since the speed of the immediate link is not
the bottleneck the NIC should be able to send at least as fast as TCP passes
packets into the queue.

A mismatch of assumptions happens in wireless networks, however, when the
immediate link \textbf{is} often the bottleneck of the path. The NIC will take a
packet out of the queue, use carrier sense and potentially retry some number of
times before actually sending the packet. It will, however, very rarely actually
drop a packet because it usually succeeds in sending the packet before reaching
its retry limit. Since TCP does not see any loss, it will assume that the path
is not congested, and so it will continue to grow its congestion window.  In
fact, the loss that TCP does see will likely \textbf{not} be related to
congestion, causing it to back off its congestion window incorrectly. % WHY??

% TODO: mention MAC layer fairness?
Even this is not really a problem for regular TCP flows; since the NIC always
has packets to send, the throughput will be kept as high as possible, and the
MAC layer will ensure fairness between clients. It will, however, still fill the
send queue with packets that the NIC can't send. This is a problem because any
packets send by other flows on the same host will now be queued behind these
packets, increasing delay significantly, which is a problem for flows where
latency is more important than throughput such as VoIP.\@  To mitigate precisely
this problem, TCP Small Queues has been implementd in the Linux kernel,
effectively hard capping the amount of bytes any TCP flow can keep in the IP
send queue at any given point in time. Although this will not solve the
underlying problem, it will limit the amount of delay a misbehaving TCP flow can
inflict on other flows.

For Multipath TCP with Coupled congestion control, however, this becomes a
problem. As explained in section COUPLED-BACKGROUND, Coupled tries to not be   % TODO: Fix section reference
too aggressive on any one link by throttling the
growth of the congestion window.
This works correctly if the congestion window
directly impacts how often the NIC puts packets on the network as throttling the
growth of the congestion window will effectively lead to other clients on the
network getting a larger share of the network capacity. When the congestion
window is sufficiently large that the NIC always has packets to send, however,
this throttling has no effect at all. The NIC is always sending, regardless of
how quickly the congestion window grows beyond that point, which means that each
\textbf{subflow} is only limited by the MAC layer fairness, which is fair
\textbf{per interface}. Thus, Coupled will end up behaving exactly like New
Reno, which is unfair when multiple subflows are used.

This problem is not related to the amount of interference between the different
wireless networks, but rather to the feedback (or rather lack thereof) given on
wireless networks when the link capacity is reached. It is also only present
when the immediate link is congested as any bottleneck further along the network
path is likely to have a queue which will overflow, drop a packet, and thus
signal to TCP that it should back off. In fact, this is precisely what we see in
the downlink case where the queue in the AP effectively conveys how congested
the wireless link is.

\begin{figure}[h]
 \centering
 \subfloat[][CDF] {\
   \scalebox{0.55}{\input{graphs/fairness-rtt-up-cdf.tex}}\label{graph:fairness-rtt-up-cdf}
 }
 \subfloat[][Multipath TCP time plot] {\
   \scalebox{0.55}{\input{graphs/fairness-rtt-up-close.tex}}\label{graph:fairness-rtt-up-close}
   % TODO: congestion window is not /10 here, throughput is *10
   % update oddities section calculation to match this
 }
 \caption{Uplink test with 100ms added to RTT}\label{graph:fairness-rtt-up}
\end{figure}

Figure~\ref{graph:fairness-rtt-up} shows a test in which the RTT was
artificially increased by 100ms, by adding delay at the IP queue level on the
test server. In this case, we would not expect the higher RTT to significantly
affect overall throughput, but rather to lower the rate of growth of the
congestion window during slow-start. The CDF, however, shows that throughput is
drastically reduced and is lower than New Reno.

To understand what is happening, consider the time plot of one of the Multipath
TCP flows in figure~\ref{graph:fairness-rtt-up-close}. The ``Bandwidth delay
product'' line shows the bandwidth delay product (and thus the target congestion
window size) of the real RTT + 100ms times the throughput observed without an
inflated RTT.\@ Clearly, the congestion window is never approaching this value.
The same is the case for the other subflow and the two parallel flows. By
looking at the number of tranmission failures reported by the NIC, we can see
that the amount of dropped packets is similar to the number observed in tests
without artificially high RTT.\@ This kind of constant loss causes both Coupled
and New Reno to continuously halve their congestion windows, and with the higher
RTT, the additive increase is too slow to bring the congestion window up to the
target size. Since TCP is now putting packets into the send queue much slower
than the NIC can transmit them, the throughput is now limited by the congestion
window, and since the window is too small, the throughput suffers.

Coupled also suffers noticably more from this problem that New Reno. This is
because Coupled is trying to behave fairly to other flows, but is reacting to
the wrong inputs.  Coupled reduces its aggressiveness if it detects that there
are competing flows, and this is again deduced from the loss rate. Since the
loss rate is close to constant, Coupled believes that there are other competing
flows, and thus reduces its aggressiveness. This further limits the growth of
each subflow's congestion window, which exaggerates the problem of the
congestion window being too small, further reducing throughput.

% TODO: mention that this is the case for certain non-wireless networks as well?
% TODO: should we discuss possible solutions here or somewhere else?
