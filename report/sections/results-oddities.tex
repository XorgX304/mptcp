Throughout our experiments, we have come across several rather odd results,
correlations and trends that we could not immediately explain. Some of these
turned out to be glitches, but many made complete sense after some hard
thinking. This section aims to explain most of the oddities that can be observed
in some of the graphs given above.

\subsubsection{Congestion window and the send queue}
Most of the time plots above show the send queue size hugging the congestion
window size through the entire test. Since we were seeing so little loss in many
of our tests, and thus the congestion window was clearly growing larger than it
should be, we did not find this strange at all, since most of the bytes
of the congestion window was bound to be in the host's send queue. Given this
assumption, we would expect the send queue to be closely following the
congestion window, with a small gap between them to cater for any packets in
flight.

After a while, however, we noticed that we were seeing the send queue following
the congestion window also when we \textbf{were} seeing loss and the congestion
window was \textbf{not} inflated. In these cases, there should not be many
packets in the send queue at all; they should mostly all be in flight.

Our first guess was that the send queue size included the TCP socket buffer
size, but this was quickly discarded as the TCP socket buffer should be close to
full (i.e.\ a constant value) the entire time thanks to netperf adding packets
in tight loop. That was clearly not what was happening here.

It turns out that the reason for this is surprisingly simple; the send queue
size reported by the kernel includes \textit{unacked packets}. TCP keeps these
around because it might have to resend them, so until they have been ACKed, they
will continue to take up space in the queue. This explains both why we were not
seeing a gap between the send queue size and the congestion window in the
no-loss experiments, \textbf{and} why the send queue was seemingly following the
congestion window when loss was occurring.

\subsubsection{Inflated RTT}
Many of our results show very high RTTs despite the network being relatively
fast (i.e.\ we see an RTT of $\approx 7$ms with \texttt{ping}). In fact, the
RTTs are so high that they are plotted as 10s of ms in the plots to fit them in
the range $[0-100]$.

The smoking gun here is that the RTT seems to increase and decrease with the
size of the send queue. As observed in section SOMETHING, the lack of loss     % TODO: set correct section reference
causes an unbounded growth of the congestion window, but with most of the
packets stuck in the send queue. Since TCP estimates RTT based on when the
packet was put into the send queue, \textbf{not} when it is actually send by the
NIC, the RTT estimate will include the time a packet spent in the queue. Since
the queue is growing, so will the RTT.

\subsubsection{Logarithmic growth of congestion window}
When looking at the growth of the congestion window, it is clear that it shows
something resembling logarithmic growth rather than the familiar linear
sawtooth. To understand why this is happening, it is necessary to look closer at
how the congestion window is increased.

The new reno congestion control used by MPTCP tries to increase the congestion
window by one MSS per RTT.\@ It does this by increasing the congestion window by
$\sfrac{1}{cwnd}$ per received ACK.\@ This works well when the assumption that
increasing the congestion window will cause you to send more packets, and thus
receive more ACKs per RTT, but falls apart when the link layer masks loss.

What happens when TCP does not see loss, as discussed in section SECTION, is   % TODO: set correction section reference
that the congestion window is usually larger than what the NIC can handle
already, and so increasing it will not cause any more packets to be sent, and
thus the number of ACKs received in an RTT will remain constant. The
$\sfrac{1}{cwnd}$ term on the other hand will grow smaller, and thus the growth
of the congestion window relative to the congestion window size per RTT will
decrease, leading to the logartichmic growth we see.

\subsubsection{Congestion window and throughput}
In figure 2013-08-14/100+100/parallel we can observe another very strange      % TODO: insert correct figure reference here
phenomenon that occured in a number of experiments. Here, we see a curious
correlation between throughput and the congestion window size. Other tests also
show a correlation between then two, but it is particularly evident in this one
as the throughput looks like it is actually bounded by the congestion window.
This struck us as very odd since the congestion window is plotted in 10s of
kilobytes, whereas the throughput is plotted in Mbits.

To determine why this was happening, we first tried to find commonalities
between the graphs that were showing this peculiar trend. It turns out that we
were only seeing this in tests we had a high RTT with constant loss rates or
high amounts of loss. These cases both share the feature that the congestion
window is constrained from growing to the full bandwidth delay product of the
link, and thus no queues are expected to build up anywhere in the network. The
limiting factor for the throughput is the congestion window not allowing TCP to
put more packets into the send queue, even though the link is ready to send. The
net effect of this is that the throughput is limited by the congestion window;
whenever the congestion window grows, the throughput increases because TCP is
allowed to put more packets on the wire. If the congestion window is halved, TCP
stops sending packets pretty much immediately, and the throughput drops.

This still did not explain why we were seeing close to a 1:1 correspondence
between the congestion window and the throughput in the 100ms RTT test. After
some digging, it turns out that this happens because of a curious case of two
different scales accidentally lining up. Remember that the congestion window is
plotted as 10s of kilobytes, and thus we plot $c(x)$ as

\begin{align*}
  c(x) = cw_x \text{\ (B)}
       = \frac{cw_x}{1024} \text{\ (kB)}
       = \frac{cw_x}{10 \cdot 1024} \text{\ (10 kB)}
\end{align*}

now, consider the throughput $t'(cw)$ allowed by a congestion window of size
$cw$ over a link with RTT $\approx 100ms$:

\begin{align*}
  t'(cw) &= \frac{cw}{0.100}                       &&\text{(Bps)} \\
         &= \frac{cw}{0.100 \cdot 10 \cdot 1024}
          = \frac{cw}{1024}                        &&\text{(MBps)} \\
         &= \frac{cw}{8 \cdot 1024}                &&\text{(Mbps)}
\end{align*}

Since the congestion window is not allowed to grow to the bandwidth delay
product as explained above, we expect the real throughput $t(x)$ to be
approximately limited by $t'(cw_x)$. It is pretty clear from the above that
$t'(cw) = 0.8 c(x)$, and so in this particular setup, we would expect to
see the throughput follow the congestion window closely at these particular
scales because $t(x) \approx 0.8 c(x)$.
