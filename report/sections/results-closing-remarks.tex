Throughout our experiments, we have come across several rather odd results,
correlations and trends that we could not immediately explain. Some of these
turned out to be glitches, but many made complete sense after some hard
thinking. This section aims to explain most of the oddities that can be observed
in some of the graphs given above.

\subsubsection{Curing inflated congestion windows}
The issue observer in \S\ref{sec:results-fairness} is one that will affect any
connection where the bottleneck link is the immediate link a client is connected
to, and as mentioned, it will affect both TCP and Multipath TCP, albeit with
differing effects. In both cases, the congestion window rarely be cut in half,
and thus will grow indefinitely beyond the ``correct'' value of the paths
bandwidth delay product.

When the congestion window grows too large, TCP and Multipath TCP alike will
continue to assume that they are being fair by continuing to put packets onto
the send queue, even though these packets are drained slower than they are being
put in. This is what is causing the bloated queue we saw in the graphs. For TCP,
this is not a problem beyond adding delay for any other flows on the host as
their packets will be queued behind an abnormally large amount of packets from
the bloated TCP flow. For Multipath TCP on the other hand, this is a problem as
Multipath TCP will then never yield the link voluntarily (because it will always
have a packet in the queue to send).

TCP Small Queues, which is the currently applied ``fix'' in the Linux kernel
does not address the underlying problem, as it simply bounds the number of
packets any given TCP flow can have in the queue at any given point in time to a
rather arbitrary number, which has the (desired) side-effect of reducing the
amount of latency that a misbehaving TCP flow could potentially introduce for
other flows on the same host. Without tuning the TSQ limit, the latency will
still not be reduced to the "correct" levels, and even if a more conservative
static limit is sent, this will also be incorrect for links with differing
speeds and RTTs.

We believe that the real way of overcoming this must be to modify TCP so that it
realizes that there is congestion (or that it is hitting the capacity of a link)
without relying solely on packet loss. Altering TCP is something that should not
be done lightly, and the larger ring effects of any change need to be examined
on a large scale before they are put into production use; that said, we have
come up with a modification to TCP that could potentially solve this issue once
and for all. The intuition behind it is that the growing size of the local queue
could be used as a signal to TCP that there is congestion somewhere without
there being packet loss.

Our observation is that the amount of packets for a particular TCP connection in
the send queue should usually be around 0, but may occasionally grow slightly if
the immediate link is temporarily busy. To determine if the queue is growing
when it should not be, we need a way to determine how large it is reasonable
that it might grow to. Our thinking is that the queue is unlikely to grow much
larger than the amount of bytes that could be transferred on the bottleneck link
in one variance of the RTT.\@ For cases when the bottleneck link is the
immediate link, we believe this should reflect a maximum value for how much we
should have to wait for the immediate link to become available:

\begin{align}
  QL = \frac{var \cdot cwnd}{RTT}\label{eq:ql}
\end{align}

Where $QL$ is the expected limit of the send queue, and $var$ is the RTT
variance measured by TCP.\@ For links where the bottleneck is not our immediate
link, however, equation~\ref{eq:ql} may not behave as desired; as the sending
host will now attempt to balance its local queue according to the queue of the
bottleneck link, potentially occasionally leaving the bottleneck link without
data to send, which would be bad for throughput. This may be overcome by
measuring the send delay variance of the immediate link directly and
substituting that for $var$ in equation~\ref{eq:ql}.

Building on the intuition that $QL$ would represent the maximum expected send
queue size, we believe that it would be appropriate to consider the send queue
exceeding that limit as a congestion event, and thus halve TCP's congestion
window. We believe this would bring back balance to TCP congestion window.

This begs the question of how often this should be allowed to happen. In theory,
a single RTT should be enough to severely drain the send queue (if it was not
bloated in the first place), and thus we think making this check once per RTT
should be appropriate.

To make it easier to integrate this with TCP, one could also implement this as a
``once per ACK'' operation just like how New Reno increases the congestion
window. We want one RTT worth of ACKs to decrease $cwnd$ by half, and so:

\begin{align*}
  \frac{cwnd}{MSS} \cdot step = \frac{cwnd_i}{2} \\
  step = \frac{MSS}{2} \cdot \frac{cwnd_i}{cwnd}
\end{align*}

Where $cwnd_i$ is set to $cwnd$ any time the send queue is smaller than $QL$.
Thus, if we do the following on every received ACK:

\begin{verbatim}
  if length(send queue) > QL
    cwnd -= step
  else
    cwnd_i = cwnd
\end{verbatim}

We believe this might correct TCP's congestion window size, bringing it back to
the point where the send queue hovers around 0. Being dynamic, it would also be
a superior solution to TCP Small Queues. It would possibly also fix the fairness
issue we've seen with Multipath TCP over feedback-less links as the congestion
windows would now be correct again.

Due to time constraints, we have not been able to implement nor test this
solution, and thus it is presented solely as a starting point for future work
trying to fix TCP when used in conjunction with wireless networks.

\subsubsection{Wireless experiments}
Running every test many times has proven very important throughout this project.
For the uplink New Reno 2.4 GHz tests for example, we initially suspected that
Multipath TCP was consistently seeing slightly lower throughput compared to
parallel, and we surmised that this might be because two WiFi interfaces
connected to one machine might somehow lead to more cross-interface interference
than with the same interfaces connected to different machines equally far apart.
After running more experiments to confirm, however, we quickly got contradicting
results where Multipath TCP was consistently faster than parallel. On balance,
we consider the results to pretty much even out across many tests. The slight
discrepancies of some links in those figures are simply due to WiFi interfaces
being extremely sensitive to their exact positioning, and average out over a
larger set of tests.

\subsubsection{Inflated RTT}
Many of our results show very high RTTs of around 600 ms despite the network
being relatively fast (i.e.\ we see an RTT of $\approx 7$ ms with
\texttt{ping}). 

% TODO: set correct section 
The RTT seems to increase and decrease with the size of the send queue. As 
observed in \S\ref{sec:results-fairness}, the lack of loss reference causes 
unbounded growth of the congestion window, but with most of the packets stuck in 
the send queue. Since TCP estimates RTT based on when the packet was put into 
the send queue, \textbf{not} when it is actually sent by the network interface, 
the RTT estimate will include the time a packet spent in the queue. Since the 
queue is growing, so will the RTT.

\subsubsection{Logarithmic growth of congestion window}

\begin{figure}[h]
 \centering
 \input{graphs/logarithmic.tex}
 \caption{Unbounded congestion window growth}\label{graph:logarithmic}
\end{figure}

Figure~\ref{graph:logarithmic} shows the congestion window size plotted over
time for one test we ran in which we were seeing very little loss. It clearly
shows something resembling logarithmic growth rather than the familiar linear
saw-tooth. To understand why this is happening, it is necessary to look closer
at how the congestion window is increased.

The congestion control mechanisms used by Multipath TCP and regular TCP try to
increase the congestion window by one MSS per RTT.\@ It does this by increasing
the congestion window by approximately $\sfrac{1}{cwnd}$ per received ACK.\@
This works well in the assumed case where increasing the congestion window will
cause you to send more packets, and thus receive more ACKs per RTT, but is not
correct when the link layer masks loss.

What happens when TCP does not see loss, as discussed in 
\S\ref{sec:results-fairness}, is %TODO: set correction section reference 
that the congestion window usually grows larger than what the network interface 
can handle, and so increasing it will not cause any more packets to be sent. 
The number of ACKs received in an RTT will therefore remain constant. The 
$\sfrac{1}{cwnd}$ term will grow smaller, and so the growth of the congestion 
window relative to the congestion window size per RTT will decrease, leading to 
the logarithmic growth seen in the results.

\subsubsection{Congestion window and the send queue}
\label{sec:closing:sendq}
Figure~\ref{graph:logarithmic}, and some other plots in this report, show the
send queue size tracing the congestion window size through the entire test.
Since we were seeing very little loss in many of our tests, and thus the
congestion window was clearly growing larger than it should be, we initially
reasoned that most of the bytes of the congestion window were likely to be in
the host's send queue. In this case, we would expect the send queue to closely
follow the congestion window, with a small gap between them representing the
packets actually in flight.

However, we then noticed that we were seeing the send queue following the
congestion window also when we \textbf{were} seeing loss and the congestion
window was \textbf{not} inflated. In these cases, there should not be many
packets in the send queue at all; they should mostly all be in flight.

Our first guess was that the send queue size included the TCP socket buffer
size, but this was quickly discarded as the TCP socket buffer should usually be
close to full (i.e.\ a constant value) due to the NetPerf tool adding packets in
a tight loop. That was clearly not what was happening here.

It turns out that the reason for this is surprisingly simple; the send queue
size reported by the kernel includes \textit{unacked packets}. TCP keeps packets
after sending in case they must be re-transmitted, so until they have been
ACKed, they will continue to take up space in the queue. This explains both why
we were not seeing a gap between the send queue size and the congestion window
in the no-loss experiments, \textbf{and} why the send queue was seemingly
following the congestion window when loss was occurring.

\subsubsection{Congestion window and throughput}

In Figure~\ref{graph:fairness-rtt-up-close}, we can observe another very strange
phenomenon that occurred in a number of experiments. Here, we see a curious
correlation between throughput and the congestion window size. Other tests also
show a correlation between then two, but it is particularly evident in this one
as the throughput looks like it is actually bounded by the congestion window.

To determine why this was happening, we first tried to find commonalities
between the graphs that were showing this peculiar trend. It turns out that we
were only seeing this in tests which showed either a high RTT with constant loss
rates, or a high amount of loss. These cases both share the feature that the
congestion window is constrained from growing to the full bandwidth delay
product of the path, and thus no queues are expected to build up anywhere in the
network. The limiting factor for the throughput is the congestion window not
allowing TCP to add more packets to the send queue, even though the link is
ready to send. The net effect of this is that the throughput is limited by the
congestion window; whenever the congestion window grows, the throughput
increases because TCP is allowed to put more packets on the wire. If the
congestion window is halved, TCP stops sending packets almost immediately, and
the throughput drops.
% vim:textwidth=80:colorcolumn=80:spell:
