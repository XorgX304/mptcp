In order to establish how well Multipath TCP works in its default configuration
with multiple WiFi interfaces, we performed a number of experiments to
measure its behaviour in various wireless environments.

We first outline the project management strategy used in \S\ref{sec:met:pm}, 
the testing setups used for the experiments in \S\ref{sec:met:setups}, 
and the equipment and tools used in \S\ref{sec:met:equip}.

\S\ref{sec:met:scripts} then gives an overview of the different scripts used for
running, analysing and graphing tests. Finally, \S\ref{sec:met:metrics}
describes the data sources used in our results.

\subsection{Project management strategy}
\label{sec:met:pm}
As we had limited time to complete the project, we decided on a small set
of initial experiments to run. These experiments were intended
to cover a wide range of test cases at a high level, and allowed us to determine
which areas would be particularly interesting for us to explore further.
We also used these tests to better understand the data we were seeing, and
fine tune our test setups and metrics. Once we determined the cases we would
investigate more in-depth, we planned sets of related tests which could
each be completed over a few days to a week.

Due to the nature of the experiments, the majority of our time was spent
running tests with all group members present. This also involved a great
deal of discussion about our results. We set specific times each week to
meet more formally to discuss our progress, and to decide what direction we would
take with future experiments. During this time, we also met regularly with
our supervisor to discuss progress and our interpretation of results.

We initially separated this report into sections, which were distributed to
each group member. We worked on individually allocated sections, and
then rotated them  for revision by the other group members. Each section was
rotated and revised many times during the final write up, until all group members
were satisfied with the final document. We used video calling during this time
to discuss changes to the report.

All test data from our experiments was shared using the Dropbox file sharing
service, which allowed us to distribute the test results in a simple way. This also
provided us with automatic backups of the test data.

The various test scripts, and the report itself, were tracked using the git
version control system. As mentioned previously,
this allowed us to work on different sections simultaneously and keep a
history of changes.

\subsection{Experiment setups}
\label{sec:met:setups}
The various experiment configurations we used are described in this section. Each
test was generally carried out using both overlapping and non-overlapping pairs of
channels in the 2.4 GHz band, and with one channel in the 2.4 GHz band and another in the
5 GHz band.

The 2.4 GHz band is more prevalent in real world deployments and so, due to
time constraints and as a consequence of this, we did not perform any tests using
multiple channels in the 5 GHz band only.

\subsubsection{Physical layout}
In all the static tests, the wireless clients were placed next to each other
approximately 60cm apart. The APs were located 2-3 metres away, separated by
about 30cm.

All tests were run in the Computer Science department building. The environment
contains with glass/plaster walls and concrete floors, and several other wireless
networks were present. Other wireless experiments were
also being coducted elsewhere in the building, but we believe these did not severely
affect our results.

\subsubsection{Sequential TCP tests}
\label{sec:met:setups:seqtcp}
This test setup consisted of running five tests consecutively, and then
plotting all of the results in a single graph. For each test, a TCP connection was
initiated from both clients to a local test server, and data was pushed through
the connection as fast as the congestion control mechanism would allow. Two
APs were used; one hosting network A and one hosting network B. The five tests
were as follows:

\begin{enumerate}
  \item Machine A connected to network A.
  \item Machine B connected to network B.
  \item Machine A connected to network A and machine B connected to network B.
  \item Machine A connected to network A and network B.
  \item Machine B connected to network A and network B.
\end{enumerate}

The two first tests were primarily used as baselines for the other tests, to determine
how well each network performed in isolation. We will refer to these as single tests.

The parallel test was used to evaluate how well two wireless clients would perform
when transmitting simultaneously on each network. This was done in order to
measure how well Multipath TCP utilized the two networks as compared to two
entirely separate, regular TCP clients.

The fourth and fifth tests used a single machine connected to both networks at
the same time, and was performed in order to gauge how well Multipath TCP behaves
with multiple wireless links.

Running this last configuration on both machines may seem unnecessary,
but it was vital to allow us to compare the parallel performance on a
given channel to the Multipath TCP performance on the same channel. The reason
for this is that wireless performance can vary greatly between different machines and
locations. Running the dual-network test on both machines allowed us to compare
the use of network A for single, parallel and Multipath TCP without worrying
about machine discrepancies. Note that a side-effect of this is that the
Multipath TCP lines in a graph are \textbf{not} from the same test, and thus
would not be expected to match up entirely. That said, we usually observed
similar behaviour for Multipath TCP for the last two tests.

Because of this discrepancy, all of the results presented in this report which
include Multipath TCP are
from the simultaneous TCP test setup described in the following section, with
the exception of in \S\ref{sec:results-performance}.

\subsubsection{Simultaneous TCP tests}
\label{sec:met:setups:simtcp}
The simultaneous TCP tests used two APs and three
wireless clients. One client used two wireless interfaces with one
connected to each network. The two remaining clients were located on either side, and were connected to either of the two networks.

The test itself was a simple TCP streaming test as used in the sequential TCP
tests, but running on all three clients simultaneously. This setup allowed
us to more directly compare the performance of a Multipath TCP client to that of
two simultaneous clients, as well as evaluate the fairness of Multipath TCP when
other clients were using the network at the same time.

This test also avoids the cross-host comparison issue exhibited by the
sequential tests, and consequently we mainly include results from the
simultaneous tests in this report as they are more reliable.

\subsubsection{UDP interrupt tests}
\label{sec:met:setups:intudp}
UDP interrupt tests were run with a single client using two
wireless interfaces. Each wireless interface was connected to one of the APs as
in the other setups, but initially data was only transmitted on one of the
interfaces.  The other interface was alternated between being in idle and active
mode for fixed time slots of 30 seconds during the experiment.

The idea behind alternating the active periods for one of the interfaces is
that, ideally, if there is no interference between these networks, there would
be nearly no loss of throughput for the always-on interface when the alternating
interface is active. If there is interference, such as with networks on the same
2.4 GHz channel, the throughput of the always-on interface should drop
significantly while the other interface is active.

To avoid having the throughput be limited by factors other than the wireless
channels, such as congestion control, these interrupt tests were run with UDP rather
than TCP.\@ This ensured that the only limiting factor was how
often the MAC layer allowed the NIC to send.

\subsubsection{Mobility tests}
\label{sec:met:setups:mobility}
For the mobility tests in \S\ref{sec:results-reacting}, we positioned our two
2.4 GHz APs such that each AP was 4 meters from an corridor intersection. 
A client standing at the intersection thus had line of sight to both 
APs, however the APs themselves were isolated from each other and set to non-overlapping 
channels. To reduce the range covered by an AP, their transmit power was 
turned down considerably so that a client 
located at one AP could barely hear the other AP.\@ We then connected a 
client laptop to both APs and walked from one AP to the other, making a stop of 
30 seconds at either AP and at the intersection. We spent approximately 10 
seconds walking between the intersection and an AP to give Multipath TCP
enough time to adapt to the changing environment.

\subsection{Equipment and software}
\label{sec:met:equip}
The WiFi interfaces used in our experiments were 2.4 GHz Element 14 Wi-Pi dongles,
commonly found in Raspberry Pi devices. For the 5 GHz tests, a
Tenda W522U was used. These dongles proved somewhat unreliable and would
occasionally drop substantial amounts of packets or fail altogether during a
test, particularly on the uplink. For some experiments we were therefore forced
to use the built-in wireless interfaces on our laptops to perform the
experiments.  These exhibited more stable loss rates and generally did not fail
during experiments, but also made comparing results across tests more difficult.

Our APs were two 2.4 GHz Netgear ProSafe WG103, as well as a Netgear N600 which
was used exclusively for 5 GHz tests. All tests were performed using 802.11g with
WPA2 encryption enabled.

The test servers were two dedicated machines connected to the UCL internal
network using a gigabit switch. Since the APs were all 100 megabit only, there
should be no bottleneck in the network beyond the APs. We used a variety of
personal laptops as wireless clients.

To run the tests, the NetPerf performance testing tool was used to generate TCP
or UDP traffic, the netem module for qdisc allowed us to emulate network delays,
and the \texttt{ss} tool provided invaluable TCP socket information.

At the time of these experiments, both the servers and client laptops were
running the most recent version of the Multipath TCP kernel, version 0.87, which
is based on Linux 3.10.

\subsection{Scripts}
\label{sec:met:scripts}
In order to automate common tasks such as running tests and analysing data, we
developed several scripts which were then used throughout the experiments. The
most interesting ones are outlined below.

Note that many of these scripts perform magic based on what wireless networks
the host machine is connected to. For this paper, two servers were used named fry and
zoidberg, and three wireless networks were set up: bender-wifi (5 GHz), fry-wifi
and zoidberg-wifi (both 2.4 GHz). Every test involved at least one of fry-wifi and
zoidberg-wifi, and some of the scripts below use the presence of a connection to
one of them as an indicator of which server should be used for tests.

\begin{description}
  \item[mp-start and mp-congestion]
    These two scripts enable Multipath TCP on the current machine, as well as
    set the appropriate congestion control algorithm on both the local machine
    and any remote machines it might be connected to.\ \texttt{mp-start} also
    stops any other wireless connections as well as some common
    network-intensive applications such as Dropbox to prevent locally generated traffic
    from interfering with the tests.
  \item[mp-routes]
    Examines the IP addresses of all active network interfaces and sets up
    routing tables according to the Multipath TCP configuration
    instructions\footnote{http://multipath-tcp.org/pmwiki.php/Users/ConfigureRouting}.
  \item[mp-run]
    The primary testing script for our experiments. First, logs information
    about test location, connected networks, nearby wireless networks, TCP
    configuration parameters and kernel version, amongst other details. It then starts
    the \texttt{mp-stats} logging daemon to record state information during the
    experiments before running NetPerf for a configurable period of
    time. When the NetPerf test is done, the logging daemon is stopped and any
    large log files are compressed. \texttt{mp-run} also supports downlink tests
    by spawning a local NetPerf server and running the NetPerf client on one of
    the server machines.
  \item[mp-stats]
    Collects the majority of statistics during a test. By default it samples
    data every 500ms. It logs statistics from the wireless interfaces (signal
    strength, bit rate, retransmit failures), IP (bytes and packets sent) and
    TCP (queue sizes, RTT estimates, retransmits)
  \item[mp-int]
    Works much the same as mp-run, but instead of running TCP sessions with
    NetPerf on all connected interfaces, it runs a UDP\_STREAM benchmark
    continuously on one interface and periodically on the other connected
    interface. This script implements the UDP interrupt tests described in
    \S\ref{sec:met:setups:intudp}.
  \item[mp-analyze]
    Given a test directory created by \texttt{mp-run} or \texttt{mp-int},
    \texttt{mp-analyze} will extract information from various log files and
    output a simple space-separated file for each interface (and a total) with
    values for everything from throughput to bit rate. This information is then
    used by \texttt{mp-plot} or \texttt{mp-cdf} to display graphs or other
    statistical information about the data.
  \item[mp-plot]
    Given a test folder, \texttt{mp-plot} will graph every statistic generated
    by \texttt{mp-analyze} for every interface the given test was run with. It
    also performs scaling to keep all values in a 0-100 range. This will be
    discussed in \S\ref{sec:met:metrics}.
  \item[mp-cdf]
    Given tuples of test folders and APs, will calculate the CDF for each
    corresponding interface in each test and graph them using
    gnuplot\footnote{http://www.gnuplot.info/}. The script uses the statistical
    programming language R\footnote{http://www.r-project.org/} to generate this
    CDF.
  \item[mp-set]
    This script is a shortcut to avoid having to type repeated folder/AP names
    to plot certain data sets. It searches for all interfaces across tests
    connected to the same channel, and then plot each group of such interfaces
    using \texttt{mp-cdf}.
  \item[mp-merge]
    Merges the data from several test sets into a single dataset.
  \item[mp-gather]
    Simple wrapper around \texttt{mp-merge} that takes folders of test sets as
    arguments, extracts what WiFi networks were used and calls \texttt{mp-merge}
    for all related tests. For example, it can find all same-channel, coupled
    test sets in all its arguments, and merge them.
\end{description}

\subsection{Metrics}
\label{sec:met:metrics}
To understand our results, it is important to know where the data comes from.
During each test, the script \texttt{mp-stats} was run every 500ms and was the
primary source of data for every plot. The most interesting metrics are
discussed below:

\begin{description}
  \item[throughput]
    This is calculated using the difference in the number of bytes sent on each
    interface (as reported by \texttt{/proc\-/net/\-dev}) and dividing it by the
    time since the last run of \texttt{mp-stats} using timestamps logged by
    \texttt{date +\%s.\%N}.
  \item[congestion window]
    Retrieved from the command \texttt{ss -inot}, which shows TCP socket
    information. The congestion window size is reported as a number of packets,
    and so we multiply it by the MSS to get the real congestion window size in bytes.
  \item[roundtrip time]
    TCP's RTT estimate is also extracted from the output of \texttt{ss}.
  \item[send queue size]
    Printed by \texttt{ss}. This includes the size of any packets sent, but not
    yet ACKed by TCP. This is discussed in \S\ref{sec:closing:sendq}.
  \item[TCP metrics (e.g.\ timeouts)]
    This statistic is provided by \texttt{/proc\-/net/\-netstat} across all
    interfaces in a machine.
\end{description}

In order to make multiple metrics easier to plot on a single graph, many of the
graphs shown in this report are drawn using our \texttt{mp-plot} script, which
scales values to keep them in the range $[0,100]$. This scaling is important to
understand in order to interpret the results correctly. For example, throughput
is measured in Mbps; utilization is a measure of what percentage of total
throughput is sent through each interface and is displayed such that 80 is 0\% and
100 is 100\%, and congestion window and send queue are both plotted in
kilobytes. The CDF graphs were drawn using the \texttt{mp-cdf} script which is
detailed in \S\ref{sec:met:scripts}.
% vim:textwidth=80:colorcolumn=80:spell:
