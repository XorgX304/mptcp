\subsection{TCP}
The Transmission Control Protocol (TCP) is one of the core internet protocols.
It provides reliable, in-order delivery of data (a stream of bytes) between two
systems, it also offers a full-duplex service meaning that it allows a pair of
byte streams to flow, one in each direction, per TCP connection. TCP is a
packet-based protocol, and uses positive acknowledgement of packets to signal
successful delivery. TCP has many other features such as, supporting
demultiplexing, which allows various application processes on any given host to
share the underlying network, and flow and congestion control which will be
explained in more detail later in this section.

TCP supports connections between any two systems in the Internet, it is an
end-to-end protocol, but in order to do this it requires an explicit connection
establishment phase. This phase is called the "three-way handshake" and is where
the communicating parties agree to exchange data. It also has a connection
teardown phase (both parties can free their allocated resources). When the
connection is set up TCP buffers enough bytes from the sending process to create
a packet and sends it to the receiver. The packet has many fields to make sure
it reaches the correct destination such as, sequence number which contains the
first byte of data carried in the packet, source/destination address, checksum
to ensure correctness and many others. All these fields play a very important
role in routing the packet correctly to its destination.

As previously mentioned TCP is a packet-based protocol, so after establishing
the connection between two communicating parties and creating the packets which
will be sent it will use flow and congestion control mechanisms to manage how
packets are exchanged throughout the network. Upon receiving packets, routers
and other devices in a network must assign some buffer space for these incoming
packets, in order to process them before forwarding along the path. Due to heavy
load or low outgoing link rate these buffers can become quite large resulting in
a queue; hence these routers become a bottleneck along the path. There is a risk
that the available buffer at the bottleneck may be filled if data is sent at a
faster rate than the device is able to forward or process it. This can result in
packets being dropped, and potentially the overload of the network. This is one
of the main problems which TCP must overcome, and the protocol implements flow
control and congestion control mechanisms to share bottlenecks fairly and avoid
overload.

Flow control is used to avoid sending data at a faster rate than the receiving
host is able to process. This is implemented using a sliding window algorithm
representing the amount of buffer space the receiver is able to commit to a
flow, and is advertised by the receiver using a field in each packet
acknowledgement sent. The sender may only keep one window of data in flight at
any given time, and so avoids overloading the receive buffer.

Congestion control is another rate-limiting mechanism which is used to avoid
overloading devices in the middle of the network path. Ideally, each TCP flow
will gain an equal share of the capacity available at the bottleneck link. A TCP
flow must also sense congestion, where the queues at the bottleneck are
overloaded, and back off its send rate accordingly.

The canonical congestion control algorithm is TCP New Reno. This algorithm
maintains a congestion window, similar to the receiver's flow control window,
which specifies the number of packets which may be in flight concurrently. The
sender must respect both the flow control and congestion windows in determining
an appropriate send rate.

TCP New Reno flows begin in a slow start phase. An initial small congestion
window is chosen, and this is increased by one maximum segment size (MSS) for
each packet acknowledgment received. This has the effect of increasing the
congestion window size exponentially until a congestion event occurs; this can
either be a time out or three duplicate ACKs. 

The congestion event halves the congestion window and at this point TCP New Reno
exits the slow start phase and enters an additive-increase phase; where the
congestion window is increased by one MSS per round-trip time, which allows it
to expand into any additional capacity which becomes available as other flows
complete.

(I feel that these 2 paragraphs are a bit confusing)

TCP New Reno implements two mechanisms for backing off when congestion is
detected. After a missed packet acknowledgement, the algorithm returns the
congestion window to its small initial size and continues in slow start.
Additionally, a fast recovery mechanism is used when temporary congestion
occurs. If three duplicate acknowledgements are received indicating a missed
packet, the congestion window is halved and additive-increase continues.

The repeated behaviour of probing for available capacity and then reducing the
window size when congestion occurs produces a distinctive saw-tooth of the
congestion window size.

\subsection{WiFi and Interference}
The 802.11 standards are a set of physical layer and MAC specifications for
implementing WiFi networks.  802.11 networks commonly operate on frequencies in
the 2.4 GHz and 5 GHz bands, which are divided into a number of overlapping
sub-bands or more commonly known as channels. For example, the 802.11b channels
in the 2.4 GHz band are 22 MHz wide and spaced 5 MHz apart, beginning with 2.412
GHz. Given that the 2.4 GHz and 5 GHz bands are not specifically reserved for
WiFi makes it more susceptible to noise as other devices such as microwave ovens
and bluetooth enabled electronics can interfere with the WiFi network.
Furthermore, a typical WiFi deployment would consist of several stations usually
connected to the network via an access point (AP). The greater the number of
stations connected to an AP on a channel, the higher the probability that the
stations would interfere with each other as they contend for the access to the
medium in order to transmit data. Interference as a results of other stations on
the network is undesirable as it leads to lower network utilization and hence
lower throughput.

In order to overcome interference with other stations transmitting on the same
channel, or a nearby channel, 802.11 implements carrier sense and random
back-off. Before transmitting, a station will sense the medium to determine if
another station is currently transmitting a data frame. If the medium is busy,
the station will defer for a random period of time and retry. This behaviour is
also used when multiple stations begin transmitting at the same time, and
collide. The random back-off selected by each station reduces the probability of
the stations interfering with each others transmission on the next retry and
this by extension ensures fairness.

The 802.11 MAC layer implements packet acknowledgements separately from TCP.\@
Acknowledgment packets are sent after each data frame is successfully received.
A station will generally retry transmitting a fixed number of times without
receiving acknowledgment before dropping a packet.

\subsection{MultiPath TCP}
Devices with multiple network interfaces are common. Many consumer smart phones
have both Wi-Fi and 3G interfaces, and data centres networks are often deployed
in topologies such as FatTree, where equipment racks are connected through
multiple paths. Data centres themselves are often multihomed to improve
reliability, meaning that they have several points of access to the wider
internet. Multipath TCP (MPTCP) is an extension to TCP currently being
standardised by the IETF, which aims to improve redundancy and throughput by
taking advantage of multiple paths for a single TCP flow. In order to achieve
this, Multipath TCP adds additional subflows to a TCP connection such that the
network interfaces of each host are connected in a fully-connected mesh.

Multipath TCP has introduced Coupled congestion control, which aims to behave
fairly when links are shared by other TCP or Multipath TCP flows. More
specifically, a single Multipath TCP flow should not gain more throughput than a
competing TCP flow simply because it has multiple subflows. However, the Coupled
algorithm also aims to utilise the available links fully when they would
otherwise be idle, meaning no other competing flows are sending.

The Coupled algorithm maintains a separate congestion window for each subflow
and uses the same congestion avoidance mechanisms as TCP New Reno, but links the
additive increase across all subflows to ensure fairness.

For each packet acknowledgment received on subflow i, the congestion window is
increased by

\begin{align*}
  cwnd_i &= cwnd_i +
    \min\left(\frac{\alpha}{cwnd_\text{total}}, \frac{1}{cwnd_i}\right) \\
  \intertext{where}
  \alpha &=
    \frac{cwnd_\text{total} \cdot \max_i\left(\frac{cwnd_i}{rtt_i^2}\right)}
         {(\sum_i \frac{cwnd_i}{rtt_i})^2}
\end{align*}

Alpha is a parameter which controls how aggressive the MPTCP flow should be in
increasing its total send rate. Alpha is chosen such that the aggregate
throughput across all subflows is equal to the throughput a TCP New Reno flow
would gain on the best of the paths available. This ensures fairness with
competing TCP flows, where there may be a shared bottleneck at some point in the
network path. Additionally, the algorithm forces the congestion window of more
congested links to increase at a slower rate than less congested links. This has
the effect of shifting traffic  onto less congested paths, which helps to
balance traffic in a network.

If the available links are idle then the Coupled algorithm will allow each
subflow to use the full capacity available to it. For example, with two idle
links the aggregate throughput of an MPTCP flow will be the capacity of both
links combined.

% TODO: The following one and a half paragraphs are a draft of a "nicer"
% explanation of Coupled congestion control; of the intuition behind it in a
% sense

However, it is important to understand \textbf{why} Coupled is expected to
eventually fill the link. In order to do that, one must understand how Coupled
tries to be fair in the first place. As the throughput of a Coupled flow
approaches the throughput a New Reno flow would get on the best link available
to the flow, Coupled gradually decreases the aggressiveness of each of its
subflows; this aggressiveness is controlled by the Alpha parameter (as
previously explained). It does this by effectively decreasing the growth rate of
each flow's congestion window. This means that the flow will still continue to
try to use more and more of the link's capacity, but it will do so very slowly.
This means that should any other flow appear, its window will grow faster than
the Coupled flow's window, and when a congestion event occurs, the window size
of both the TCP flow and Multipath TCP flow will be halved the same way. This
phenomenon happens because the Coupled algorithm (Linked increases) does not
allow perfect resource pooling, in other words, it couples the increase rate of
all Multipath TCP subflows but keeps a separate congestion window for the
decrease, this measure ensures fairness by avoiding flappiness (which would be
seen if the algorithm was fully coupled). With perfect resource pooling the
traffic would go to the best path, but if there happened to be two paths with
the same level of congestion the traffic would enter a continuous state of
flapping between the two paths, leading to the congestion controller not
allocating any window time to other subflows (extremely unfair). By only
coupling the increase of subflows, limiting the increase rate to the maximum
increase a TCP flow would have, more window will proportionally be given to
lower loss rate subflows and the decrease will only depend on the congestion
window of each separate subflow which again will lead to Coupled being able to
probe the very lossy paths and finding the best paths more efficiently.  Due to
the Alpha parameter controlling the aggressivenes of Multipath TCP subflows and
the fact that their decrease behaves the same as New Reno this shows that the
aggregate throughput of all the subflows should only be up to the best a single
TCP flow would get on its best path. (maybe i'm stretching this a bit now or
repeating myself but hey my thought flow is not the greatest at this
time :D)  % TODO HERE

\subsection{Motivation}
Multipath TCP is a very desirable alternative to single TCP.

\begin{enumerate}
  \item Multipath TCP makes use of multiple interfaces which can be either
    cross-channel or same channel and aim to provide better throughput and
    redundancy when compared to single TCP.
  \item Multipath TCP, regardless of having been designed for wired
    environments, from seen experiments shows a significant gain in throughput
    and better redundancy.
\end{enumerate}

However, it is known that multiple clients connected to the same or nearby WiFi
channels can interfere with each other and in some cases even cross-channel
connections experience significant interference (can I say this here?). Also,
since Multipath TCP was designed for wired networks it is pertinent (or
relevant?) to investigate the effect of the underlying network on Multipath TCP
and vice-versa in a wireless setting which is very prone to interference and
quite unpredictable as it deals with other factors such as white noise, external
factors, hardware issues, and so on (Too much??).  To approach this problem we
have based our project on several experiments which will be explained in greater
detail in the following section of the report.
