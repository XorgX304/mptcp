\documentclass[12pt,a4paper]{article}
\usepackage{authblk}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{xfrac}
\usepackage[vmargin=3.5cm]{geometry}
\usepackage{hyperref}
\usepackage[lofdepth]{subfig}
\renewcommand{\arraystretch}{1.3}
\title{Cross-interface Interference and MultiPath TCP}
\author{Barlow-Bignell, J}
\author{de Silva, C}
\author{Gjengset, J}
\author{Oliha, P}
\affil{University College London}
\date{}
\begin{document}
\maketitle

\begin{abstract}
  ABSTRACT GOES HERE
\end{abstract}
\clearpage

\section{Introduction}
With the extension of Multipath TCP to regular TCP it is possible to have
multiple simultaneous connections on one device. On modern mobile devices that
usually have more than one network interface(wifi, 3G) this can gaurantee extra
reliablity making the connections more robust to loss. With the promise of
greater throughput and reliability, it is immediately easy to see usage
scenarios that involve  multiple wifi cards on a laptop in order to benifit from
MPTCP.

Interference is a bane of any wireless network and wifi is no exception to this.
Multiple devices with single wifi card often interfer with each other and rely
on 802.11 carrier-sense to ensure fair access to the wireless medium. Hence it
is only natural that multiple wifi cards on a device would lead to some degree
of interference.

The goal of this report is to investigate the degree of interference(if any)
between multiple wifi interfaces on the same device, how this interference
influences throughput and fairness to other devices on the network, and how this
influences the multipath TCP protocol. The remaining part of this report would
present a brief background on TCP, Wifi, and Multipath TCP, the methodology of
the experiments conducted, results and evaluation of these experiments and the
conclusions reached.

\section{Background}
\subsection{WiFi}
\subsection{Interference}
\subsection{TCP}
\subsection{MultiPath TCP}

\section{Methodology}
\subsection{Equipment}
The tools used to create the test environment includes four 2.4GHz Wifi USB
dongles, a 5GHz Wifi USB dongle, three wireless basestations; two operating on
the 2.4GHz channel and one dual-band on 2.4GHz and  5GHz channel, two server
machines and two laptop PCs. At the time of this experiment, both the servers
and personal computers were running the most current version of the Linux MPTCP
enabled kernel gotten from \href{http://multipath-tcp.org/}{multipath-tcp.org}

\subsection{Experiment setups}
For most tested configurations, be it non-interfering 2.4 Ghz and 5 Ghz networks
or interfering 2.4 Ghz networks on the same channel, one of two tests setups
were used. The first setup was running the five tests outlined below
sequentially:

\begin{enumerate}
  \item Network \#1 on machine \#1 only
  \item Network \#2 on machine \#2 only
  \item Network \#1 on machine \#1 and network \#2 on machine \#2 in parallel
  \item Network \#1 and \#2 on machine \#1
  \item Network \#1 and \#2 on machine \#2
\end{enumerate}

The two first tests were primarily used as baselines for the other tests to see
how well each network performed on its own without interference from the other
network. The parallel test was added to aid in measuring cross-interface
interference without MPTCP.\@ This was necessary in order to measure how well
MultiPath TCP utilized the two networks compared to two entirely separate,
vanilla TCP clients. Originally we only ran one of the two final tests, but we
split it into two tests early on in order to be able to more directly compare
the utilization and performance of a network using MPTCP to its single and
parallel counterparts. Most of the comparisons done in this paper show
comparisons between test \#1, \#3 and \#4 or \#2, \#3 and \#5.

The other common test setup was using three machines at the same time so that
one machine was connected to network \#1, one machine to network \#2, and one
machine to both networks. All machines were running the MPTCP kernel. This setup
allowed us to more directly compare the performance of MPTCP to that of a two
simultaneous non-MPTCP clients. It also let us evaluate the fairness of MPTCP
when other clients are also using the network.

\subsection{Scripts}
In order to automate several oft-performed tasks such as running tests and
analyzing data, several scripts were developed and used throughout the writing
of this paper. The most interesting ones are outlined below.

Note that many of these scripts perform magic based on what wireless networks
the host machine is connected to. For this paper, two servers were used: fry and
zoidberg. Four wireless networks were set up, named bender-wifi, fry-wifi,
leela-wifi and zoidberg-wifi. Every test involved at least one of fry-wifi and
zoidberg-wifi, and the scripts below use the presence of a connection to one of
them as an indicator of which server will be used for tests.

\begin{description}
  \item[mp-start and mp-congestion]
    These two scripts enable MPTCP on the current machine, as well as set the
    appropriate congestion control algorithm on both the local machine and any
    remote machines it might be connected to.\ mp-start also stops any other
    wireless connections as well as some common network-intensive applications
    such as Dropbox. This prevents other traffic interfering with any running
    tests.
  \item[mp-routes]
    Examines the ip addresses of any active network interfaces and sets up
    routing tables according to
    \href{http://multipath-tcp.org/pmwiki.php/Users/ConfigureRouting}{http://multipath-tcp.org/\-pmwiki.php/\-Users/\-ConfigureRouting}.
  \item[mp-run]
    The primary testing script for MPTCP experiments. First, logs information
    about test location, connected networks, nearby wireless networks, tcp
    configuration parameters, kernel version, etc. Then it starts various
    logging daemons such as mp-stats and tcpdump to record state information
    during the experiments. It then runs netperf for a configurable period of
    time before it stops all logging daemons and compresses any large logs.

    Also supports doing downlink tests by spawning a local netperf server and
    running the netperf client on one of the server machines.
  \item[mp-stats]
    Collects the majority of statistics during a test. By default it samples
    data once every second. It logs stats from the wireless interfaces (signal
    strength, bitrate, retransmit failures), IP (bytes and packets sent) and TCP
    (queue sizes, rtt estimates, retransmits)
  \item[mp-int]
    Works much the same as mp-run, but instead of running TCP sessions with
    netperf on all connected interfaces, it runs a UDP\_STREAM test continuously
    on one interface and periodically on the other connected interace. This test
    is performed mainly to test the amount of interference between interfaces
    without the overhead of tcp.
  \item[mp-analyze]
    Given a test directory created by mp-run or mp-int, mp-analyze will extract
    information from various log files and output a simple space-separated file
    for each interface (and a total) with values for everything from throughput
    to bitrate to rtt. This information is then used by mp-plot or mp-cdf to
    display graphs or other statistical information about the data.
  \item[mp-plot]
    Given a test folder, will simply graph every statistic generated by
    mp-analyze for every interface the given test was run with. It also performs
    scaling to keep all values in a 0-100 range. Throughput is for example
    scaled to be shown in Mbps rather than Bps.
  \item[mp-cdf]
    Given tuples of test folders and APs, will calculate the CDF for each
    corresponding interface in each test and graph them using gnuplot. The
    script uses the statistical programming language R to generate the CDF (or
    technically, the ECDF).
  \item[mp-set]
    This script is a shortcut to avoid having to type repeated folder/AP
    names to plot certain data sets. It tries to find all interfaces across
    tests connected to the same channel, and then plot each group of such
    interfaces using mp-cdf.
  \item[mp-merge]
    Merges the data from several test sets into a single set. Optionally also
    does a simple form of normalization in order to make the results more
    relevant when the merged set of samples are plotted as a single CDF.\@ The
    normalization is performed by finding the average of the median throughput
    in each test in the set, and then subtracting that from every throughput
    measurement. This retains both the shape and width of the CDFs, while
    ignoring the absolute throughput values which can vary quite a lot from one
    test to another due to other clients using the network.
  \item[mp-gather]
    Simple wrapper around mp-merge that takes folders of test sets as arguments,
    extracts ap names and calls mp-merge for all related tests. For example, it
    will find all same-channel, coupled test sets in all its arguments, and
    merge them, optionally using mp-merge's normalization feature.
\end{description}
\subsection{Interrupt tests}
For the interrupt tests, each wireless interface is connected to a separate
basestation just like in the other setups, the only difference here is that one
of the interfaces alternates between being in idle and active mode while the
other interface is always active. The alternating interface is idle or active
for fixed time slots in the duration of the experiment.

The idea behind alternating the active periods for one of the interfaces is
simple; ideally if there is no interference between these interfaces then there
would not be a loss in throughput for the always-active interface when the
alternating interface is active. Given the nature of wireless signals this is a
rather utopic idea, hence a more desirable result would be one that shows
minimal drop in throughput. Minimal in this scenario means that each interface
should achieve at least 90\% of its throughput in a single interface setup.
However, the graph of figure FIG shows that ther is some considerable drop in  % TODO: insert correct figure reference
throughput when the alternating interface is switched-on thereby indicating the
presence on non-minimal cross-interface interference between the wireless
interfaces.

\section{Results and Evaluation}
Note that many of the graphs shown in this section are drawn using our
\texttt{mp-plot} script, which will scale values to keep them in the range
$[0,100]$. A couple of points about this scaling that are worth pointing out:
throughput is measured in Mbps; utilization is a measure of what percentage of
total throughput is sent through each interface and is displayed so that 80 is
0\% and 100 is 100\%.
\subsection{Cross-band interference} % 5 vs 2.4
\subsection{Cross-channel interference}
\subsection{Same-channel interference}
\subsection{Downlink experiments}
The setup for this test is the same as explained in the setup 3 of "Testing
Setups". Here the goal of the experiment is to analyse MPTCP's behaviour with
regards to interference, throughput and fairness to other TCP flows on the
downlink. For this experiment 2 AP's are used; one on 5GHz frequency band and
the other on 2.4GHz. To ensure that the AP's have the same amount of load, each
parallel machine connects to one AP while the MPTCP machine connects to both AP
using one of its interface for each; thereby making it seem as if each AP is
connected to 2 clients. This also applies to the two servers to which the TCP
connections would be made. Netperf is used to create downlink traffic between
the servers and client; this lasts for two minutes time slots. This is ran
multiple times and the aggregate result can be seen in figure FIG.             % TODO: correct fig (maybe graphs from 14-08-2013).

The coupled congestion control algorithm is used by all machines in this test.
For the parallel machines this is has no consequence since they only have one
TCP flow each so in essence it acts like the normal TCP congestion algorithm.
However for the MPTCP flows, coupled congestion control limits the aggregate
throughput to be as good as the throughput of the best link on that path. In
order words, the sum of the throughput should be the same as that of the single
5GHz channel. We do actually see this phenomenon from the graphs where the sum
of the MPTCP throughputs is about the same as that of the best parallel link
save for some slight variation which can be attributed to the wireless
environment in which these experiments where carried out.

From the graph we see MPTCP also tries to achieve a form of load balancing by
moving data off the congested path in this case the 2.5GHz link onto the 5GHz
link.

It should be noted that this also shows that MPTCP running the coupled
congestion control algorithm ensures fairness on the downlink. It does this with
the aid of loss feedback from the channel; so as packets build up in the AP, the
queue fills up leading to packet drops and then loss. This indicates to the
MPTCP sender to slow down the rate.

\subsection{Parallel vs. MPTCP performance}
\subsubsection{Fairness}
\subsection{MPTCP congestion control}
\subsection{Carrier Sense and cross-channel interference}
\begin{figure}[h]
 \centering
 \input{graphs/mp-int-2.4-ni.tex}
 \caption{Cross-channel interference}\label{graph:cc-interference}
\end{figure}

The cross-channel interference observed in Figure~\ref{graph:cc-interference} is
quite interesting as the throughput and utilization measurements show a
completely fair sharing between the two networks when both are active. There are
also nearly no 802.11 retransmit failures. This perfect split implies that
Carrier Sense is being employed here, effectively doing time multiplexing
between the interfaces.  This would be fine if the two networks were on the same
channel and should not transmit simultaneously, but the experiment shown in
Figure~\ref{graph:cc-interference} was performed with two networks on opposite
ends of the 2.4 Ghz spectrum (channel 1 and 11), and the two interfaces should
be able to both transmit at the same time, giving twice the throughput.

Looking at the total throughput, it is clear that some performance gain is
achieved, but clearly the gain is closer to 50\% than the 100\% one would expect
from non-interfering channels.

\begin{figure}[h]
 \centering
 \input{graphs/mp-int-2.4-i.tex}
 \caption{Same-channel interference}\label{graph:sc-interference}
\end{figure}

The results in Figure~\ref{graph:sc-interference} are perhaps even more
surprising as Carrier Sense should enforce a fairly strict time multiplexing
with two networks on the same channel, meaning the total throughput should
remain almost the same whether one or two interfaces are active. The
experimental results on the other hand show that the total throughput decreases
to almost 50\% when both interfaces are active. Clearly Carrier Sense is not
performing as it should.

\subsection{Observations on data oddities}
Throughout our experiments, we have come across several rather odd results,
correlations and trends that we could not immediately explain. Some of these
turned out to be glitches, but many made complete sense after some hard
thinking. This section aims to explain most of the oddities that can be observed
in some of the graphs given above.

\subsubsection{Congestion window and the send queue}
Most of the time plots above show the send queue size hugging the congestion
window size through the entire test. Since we were seeing so little loss in many
of our tests, and thus the congestion window was clearly growing larger than it
should be, we did not find this strange at all, since most of the bytes
of the congestion window was bound to be in the host's send queue. Given this
assumption, we would expect the send queue to be closely following the
congestion window, with a small gap between them to cater for any packets in
flight.

After a while, however, we noticed that we were seeing the send queue following
the congestion window also when we \textbf{were} seeing loss and the congestion
window was \textbf{not} inflated. In these cases, there should not be many
packets in the send queue at all; they should mostly all be in flight.

Our first guess was that the send queue size included the TCP socket buffer
size, but this was quickly discarded as the TCP socket buffer should be close to
full (i.e.\ a constant value) the entire time thanks to netperf adding packets
in tight loop. That was clearly not what was happening here.

It turns out that the reason for this is surprisingly simple; the send queue
size reported by the kernel includes \textit{unacked packets}. TCP keeps these
around because it might have to resend them, so until they have been ACKed, they
will continue to take up space in the queue. This explains both why we were not
seeing a gap between the send queue size and the congestion window in the
no-loss experiments, \textbf{and} why the send queue was seemingly following the
congestion window when loss was occurring.

\subsubsection{Inflated RTT}
Many of our results show very high RTTs despite the network being relatively
fast (i.e.\ we see an RTT of $\approx 7$ms with \texttt{ping}). In fact, the
RTTs are so high that they are plotted as 10s of ms in the plots to fit them in
the range $[0-100]$.

The smoking gun here is that the RTT seems to increase and decrease with the
size of the send queue. As observed in section SOMETHING, the lack of loss     % TODO: set correct section reference
causes an unbounded growth of the congestion window, but with most of the
packets stuck in the send queue. Since TCP estimates RTT based on when the
packet was put into the send queue, \textbf{not} when it is actually send by the
NIC, the RTT estimate will include the time a packet spent in the queue. Since
the queue is growing, so will the RTT.

\subsubsection{Logarithmic growth of congestion window}
When looking at the growth of the congestion window, it is clear that it shows
something resembling logarithmic growth rather than the familiar linear
sawtooth. To understand why this is happening, it is necessary to look closer at
how the congestion window is increased.

The new reno congestion control used by MPTCP tries to increase the congestion
window by one MSS per RTT.\@ It does this by increasing the congestion window by
$\sfrac{1}{cwnd}$ per received ACK.\@ This works well when the assumption that
increasing the congestion window will cause you to send more packets, and thus
receive more ACKs per RTT, but falls apart when the link layer masks loss.

What happens when TCP does not see loss, as discussed in section SECTION, is   % TODO: set correction section reference
that the congestion window is usually larger than what the NIC can handle
already, and so increasing it will not cause any more packets to be sent, and
thus the number of ACKs received in an RTT will remain constant. The
$\sfrac{1}{cwnd}$ term on the other hand will grow smaller, and thus the growth
of the congestion window relative to the congestion window size per RTT will
decrease, leading to the logartichmic growth we see.

\subsubsection{Congestion window and throughput}
In figure 2013-08-14/100+100/parallel we can observe another very strange      % TODO: insert correct figure reference here
phenomenon that occured in a number of experiments. Here, we see a curious
correlation between throughput and the congestion window size. Other tests also
show a correlation between then two, but it is particularly evident in this one
as the throughput looks like it is actually bounded by the congestion window.
This struck us as very odd since the congestion window is plotted in 10s of
kilobytes, whereas the throughput is plotted in Mbits.

To determine why this was happening, we first tried to find commonalities
between the graphs that were showing this peculiar trend. It turns out that we
were only seeing this in tests we had a high RTT with constant loss rates or
high amounts of loss. These cases both share the feature that the congestion
window is constrained from growing to the full bandwidth delay product of the
link, and thus no queues are expected to build up anywhere in the network. The
limiting factor for the throughput is the congestion window not allowing TCP to
put more packets into the send queue, even though the link is ready to send. The
net effect of this is that the throughput is limited by the congestion window;
whenever the congestion window grows, the throughput increases because TCP is
allowed to put more packets on the wire. If the congestion window is halved, TCP
stops sending packets pretty much immediately, and the throughput drops.

This still did not explain why we were seeing close to a 1:1 correspondence
between the congestion window and the throughput in the 100ms RTT test. After
some digging, it turns out that this happens because of a curious case of two
different scales accidentally lining up. Remember that the congestion window is
plotted as 10s of kilobytes, and thus we plot $c(x)$ as

\begin{align*}
  c(x) = cw_x \text{\ (B)}
       = \frac{cw_x}{1024} \text{\ (kB)}
       = \frac{cw_x}{10 \times 1024} \text{\ (10 kB)}
\end{align*}

now, consider the throughput $t'(cw)$ allowed by a congestion window of size
$cw$ over a link with RTT $\approx 100ms$:

\begin{align*}
  t'(cw) &= \frac{cw}{0.100}                       &&\text{(Bps)} \\
         &= \frac{cw}{0.100 \times 10 \times 1024}
          = \frac{cw}{1024}                        &&\text{(MBps)} \\
         &= \frac{cw}{8 \times 1024}               &&\text{(Mbps)}
\end{align*}

Since the congestion window is not allowed to grow to the bandwidth delay
product as explained above, we expect the real throughput $t(x)$ to be
approximately limited by $t'(cw_x)$. It is pretty clear from the above that
$t'(cw) = 0.8 \times c(x)$, and so in this particular setup, we would expect to
see the throughput follow the congestion window closely at these particular
scales because $t(x) \approx 0.8 \times c(x)$.

\subsection{What if pigs could fly?} % Optimal solution

\section{Conclusion}

%Subfig example
%\begin{figure}[h]
% \centering
% \subfloat[][cross-channel interference] {\
%   \scalebox{0.55}{\input{graphs/mp-int-2.4-ni.tex}}\label{graph:cc-interference}
% }
% \subfloat[][same-channel interference] {\
%   \scalebox{0.55}{\input{graphs/mp-int-2.4-i.tex}}\label{graph:sc-interference}
% }
%
% \caption{Interference experiments}\label{graph:interference}
%\end{figure}

\end{document}
% vim:textwidth=80:colorcolumn=80:
