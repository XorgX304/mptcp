\documentclass[12pt,a4paper]{article}
\usepackage{authblk}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[vmargin=3.5cm]{geometry}
\usepackage{hyperref}
\usepackage[lofdepth]{subfig}
\renewcommand{\arraystretch}{1.3}
\title{Cross-interface Interference and MultiPath TCP}
\author{Barlow-Bignell, J}
\author{de Silva, C}
\author{Gjengset, J}
\author{Oliha, P}
\affil{University College London}
\date{}
\begin{document}
\maketitle

\begin{abstract}
  ABSTRACT GOES HERE
\end{abstract}
\clearpage

\section{Introduction}

\section{Background}
\subsection{WiFi}
\subsection{Interference}
\subsection{TCP}
\subsection{MultiPath TCP}

\section{Methodology}
\subsection{Equipment}
\subsection{Experiment setups}
For most tested configurations, be it non-interfering 2.4 Ghz and 5 Ghz networks
or interfering 2.4 Ghz networks on the same channel, a fixed set of experiments
were run. The set consisted of the five tests outlined below:

\begin{enumerate}
  \item Network \#1 on machine \#1 only
  \item Network \#2 on machine \#2 only
  \item Network \#1 on machine \#1 and network \#2 on machine \#2 in parallel
  \item Network \#1 and \#2 on machine \#1
  \item Network \#1 and \#2 on machine \#2
\end{enumerate}

The two first tests were primarily used as baselines for the other tests to see
how well each network performed on its own without interference from the other
network. The parallel test was added to aid in measuring cross-interface
interference without MPTCP.\@ This was necessary in order to measure how well
MultiPath TCP utilized the two networks compared to two entirely separate,
vanilla TCP clients. Originally we only ran one of the two final tests, but we
split it into two tests early on in order to be able to more directly compare
the utilization and performance of a network using MPTCP to its single and
parallel counterparts. Most of the comparisons done in this paper show
comparisons between test \#1, \#3 and \#4 or \#2, \#3 and \#5.
\subsection{Scripts}
In order to automate several oft-performed tasks such as running tests and
analyzing data, several scripts were developed and used throughout the writing
of this paper. The most interesting ones are outlined below.

Note that many of these scripts perform magic based on what wireless networks
the host machine is connected to. For this paper, two servers were used: fry and
zoidberg. Four wireless networks were set up, named bender-wifi, fry-wifi,
leela-wifi and zoidberg-wifi. Every test involved at least one of fry-wifi and
zoidberg-wifi, and the scripts below use the presence of a connection to one of
them as an indicator of which server will be used for tests.

\begin{description}
  \item[mp-start and mp-congestion]
    These two scripts enable MPTCP on the current machine, as well as set the
    appropriate congestion control algorithm on both the local machine and any
    remote machines it might be connected to.\ mp-start also stops any other
    wireless connections as well as some common network-intensive applications
    such as Dropbox. This prevents other traffic interfering with any running
    tests.
  \item[mp-routes]
    Examines the ip addresses of any active network interfaces and sets up
    routing tables according to
    \href{http://multipath-tcp.org/pmwiki.php/Users/ConfigureRouting}{http://multipath-tcp.org/\-pmwiki.php/\-Users/\-ConfigureRouting}.
  \item[mp-run]
    The primary testing script for MPTCP experiments. First, logs information
    about test location, connected networks, nearby wireless networks, tcp
    configuration parameters, kernel version, etc. Then it starts various
    logging daemons such as mp-stats and tcpdump to record state information
    during the experiments. It then runs netperf for a configurable period of
    time before it stops all logging daemons and compresses any large logs.

    Also supports doing downlink tests by spawning a local netperf server and
    running the netperf client on one of the server machines.
  \item[mp-stats]
    Collects the majority of statistics during a test. By default it samples
    data once every second. It logs stats from the wireless interfaces (signal
    strength, bitrate, retransmit failures), IP (bytes and packets sent) and TCP
    (queue sizes, rtt estimates, retransmits)
  \item[mp-int]
    Works much the same as mp-run, but instead of running TCP sessions with
    netperf on all connected interfaces, it runs a UDP\_STREAM test continuously
    on one interface and periodically on the other connected interace. This test
    is performed mainly to test the amount of interference between interfaces
    without the overhead of tcp.
  \item[mp-analyze]
    Given a test directory created by mp-run or mp-int, mp-analyze will extract
    information from various log files and output a simple space-separated file
    for each interface (and a total) with values for everything from throughput
    to bitrate to rtt. This information is then used by mp-plot or mp-cdf to
    display graphs or other statistical information about the data.
  \item[mp-plot]
    Given a test folder, will simply graph every statistic generated by
    mp-analyze for every interface the given test was run with. It also performs
    scaling to keep all values in a 0-100 range. Throughput is for example
    scaled to be shown in Mbps rather than Bps.
  \item[mp-cdf]
    Given tuples of test folders and APs, will calculate the CDF for each
    corresponding interface in each test and graph them using gnuplot. The
    script uses the statistical programming language R to generate the CDF (or
    technically, the ECDF).
  \item[mp-set]
    This script is a shortcut to avoid having to type repeated folder/AP
    names to plot certain data sets. It tries to find all interfaces across
    tests connected to the same channel, and then plot each group of such
    interfaces using mp-cdf.
  \item[mp-merge]
    Merges the data from several test sets into a single set. Optionally also
    does a simple form of normalization in order to make the results more
    relevant when the merged set of samples are plotted as a single CDF.\@ The
    normalization is performed by finding the average of the median throughput
    in each test in the set, and then subtracting that from every throughput
    measurement. This retains both the shape and width of the CDFs, while
    ignoring the absolute throughput values which can vary quite a lot from one
    test to another due to other clients using the network.
  \item[mp-gather]
    Simple wrapper around mp-merge that takes folders of test sets as arguments,
    extracts ap names and calls mp-merge for all related tests. For example, it
    will find all same-channel, coupled test sets in all its arguments, and
    merge them, optionally using mp-merge's normalization feature.
\end{description}
\subsection{Interrupt tests}

\section{Results and Evaluation}
Note that many of the graphs shown in this section are drawn using our
\texttt{mp-plot} script, which will scale values to keep them in the range
$[0,100]$. A couple of points about this scaling that are worth pointing out:
throughput is measured in Mbps; utilization is a measure of what percentage of
total throughput is sent through each interface and is displayed so that 80 is
0\% and 100 is 100\%.
\subsection{Cross-band interference} % 5 vs 2.4
\subsection{Cross-channel interference}
\subsection{Same-channel interference}
\subsection{Parallel vs. MPTCP performance}
\subsubsection{Fairness}
\subsection{MPTCP congestion control}
\subsection{Carrier Sense and cross-channel interference}
\begin{figure}[h]
 \centering
 \input{graphs/mp-int-2.4-ni.tex}
 \caption{Cross-channel interference}\label{graph:cc-interference}
\end{figure}

The cross-channel interference observed in Figure~\ref{graph:cc-interference} is
quite interesting as the throughput and utilization measurements show a
completely fair sharing between the two networks when both are active. There are
also nearly no 802.11 retransmit failures. This perfect split implies that
Carrier Sense is being employed here, effectively doing time multiplexing
between the interfaces.  This would be fine if the two networks were on the same
channel and should not transmit simultaneously, but the experiment shown in
Figure~\ref{graph:cc-interference} was performed with two networks on opposite
ends of the 2.4 Ghz spectrum (channel 1 and 11), and the two interfaces should
be able to both transmit at the same time, giving twice the throughput.

Looking at the total throughput, it is clear that some performance gain is
achieved, but clearly the gain is closer to 50\% than the 100\% one would expect
from non-interfering channels.

\begin{figure}[h]
 \centering
 \input{graphs/mp-int-2.4-i.tex}
 \caption{Same-channel interference}\label{graph:sc-interference}
\end{figure}

The results in Figure~\ref{graph:sc-interference} are perhaps even more
surprising as Carrier Sense should enforce a fairly strict time multiplexing
with two networks on the same channel, meaning the total throughput should
remain almost the same whether one or two interfaces are active. The
experimental results on the other hand show that the total throughput decreases
to almost 50\% when both interfaces are active. Clearly Carrier Sense is not
performing as it should.

\subsection{Observations on data oddities}
Throughout our experiments, we have come across several rather odd results,
correlations and trends that we could not immediately explain. Some of these
turned out to be glitches, but many made complete sense after some hard
thinking. This section aims to explain most of the oddities that can be observed
in some of the graphs given above.

\subsubsection{Congestion window and the send queue}
Most of the time plots above show the send queue size hugging the congestion
window size through the entire test. Since we were seeing so little loss in many
of our tests, and thus the congestion window was clearly growing larger than it
should be, we did not find this strange at all, since most of the bytes
of the congestion window was bound to be in the host's send queue. Given this
assumption, we would expect the send queue to be closely following the
congestion window, with a small gap between them to cater for any packets in
flight.

After a while, however, we noticed that we were seeing the send queue following
the congestion window also when we \textbf{were} seeing loss and the congestion
window was \textbf{not} inflated. In these cases, there should not be many
packets in the send queue at all; they should mostly all be in flight.

Our first guess was that the send queue size included the TCP socket buffer
size, but this was quickly discarded as the TCP socket buffer should be close to
full (i.e.\ a constant value) the entire time thanks to netperf adding packets
in tight loop. That was clearly not what was happening here.

It turns out that the reason for this is surprisingly simple; the send queue
size reported by the kernel includes \textit{unacked packets}. TCP keeps these
around because it might have to resend them, so until they have been ACKed, they
will continue to take up space in the queue. This explains both why we were not
seeing a gap between the send queue size and the congestion window in the
no-loss experiments, \textbf{and} why the send queue was seemingly following the
congestion window when loss was occurring.

\subsubsection{Inflated RTT}
Many of our results show very high RTTs despite the network being relatively
fast (i.e.\ we see an RTT of $\approx 7$ms with \texttt{ping}). In fact, the
RTTs are so high that they are plotted as 10s of ms in the plots to fit them in
the range $[0-100]$.

The smoking gun here is that the RTT seems to increase and decrease with the
size of the send queue. As observed in section SOMETHING, the lack of loss     % TODO: set correct section reference
causes an unbounded growth of the congestion window, but with most of the
packets stuck in the send queue. Since TCP estimates RTT based on when the
packet was put into the send queue, \textbf{not} when it is actually send by the
NIC, the RTT estimate will include the time a packet spent in the queue. Since
the queue is growing, so will the RTT.

\subsubsection{Logarithmic growth of congestion window}
When looking at the growth of the congestion window, it is clear that it shows
something resembling logarithmic growth rather than the familiar linear
sawtooth. To understand why this is happening, it is necessary to look closer at
how the congestion window is increased.

The new reno congestion control used by MPTCP tries to increase the congestion
window by one MSS per RTT.\@ It does this by increasing the congestion window by
$\sfrac{1}{cwnd}$ per received ACK.\@ This works well when the assumption that
increasing the congestion window will cause you to send more packets, and thus
receive more ACKs per RTT, but falls apart when the link layer masks loss.

What happens when TCP does not see loss, as discussed in section SECTION, is   % TODO: set correction section reference
that the congestion window is usually larger than what the NIC can handle
already, and so increasing it will not cause any more packets to be sent, and
thus the number of ACKs received in an RTT will remain constant. The
$\sfrac{1}{cwnd}$ term on the other hand will grow smaller, and thus the growth
of the congestion window relative to the congestion window size per RTT will
decrease, leading to the logartichmic growth we see.

\subsubsection{Congestion window and throughput}
In figure 2013-08-14/100+100/parallel we can observe another very strange      % TODO: insert correct figure reference here
phenomenon that occured in a number of experiments. Here, we see a curious
correlation between throughput and the congestion window size. Other tests also
show a correlation between then two, but it is particularly evident in this one
as the throughput looks like it is actually bounded by the congestion window.
This struck us as very odd since the congestion window is plotted in 10s of
kilobytes, whereas the throughput is plotted in Mbits.

To determine why this was happening, we first tried to find commonalities
between the graphs that were showing this peculiar trend. It turns out that we
were only seeing this in tests we had a high RTT with constant loss rates or
high amounts of loss. These cases both share the feature that the congestion
window is constrained from growing to the full bandwidth delay product of the
link, and thus no queues are expected to build up anywhere in the network. The
limiting factor for the throughput is the congestion window not allowing TCP to
put more packets into the send queue, even though the link is ready to send. The
net effect of this is that the throughput is limited by the congestion window;
whenever the congestion window grows, the throughput increases because TCP is
allowed to put more packets on the wire. If the congestion window is halved, TCP
stops sending packets pretty much immediately, and the throughput drops.

This still did not explain why we were seeing close to a 1:1 correspondence
between the congestion window and the throughput in the 100ms RTT test. After
some digging, it turns out that this happens because of a curious case of two
different scales accidentally lining up. Remember that the congestion window is
plotted as 10s of kilobytes, and thus we plot $c(x)$ as

\begin{align*}
  c(x) = cw_x \text{\ (B)}
       = \frac{cw_x}{1024} \text{\ (kB)}
       = \frac{cw_x}{10 \times 1024} \text{\ (10 kB)}
\end{align*}

now, consider the throughput $t'(cw)$ allowed by a congestion window of size
$cw$ over a link with RTT $\approx 100ms$:

\begin{align*}
  t'(cw) &= \frac{cw}{0.100}                       &&\text{(Bps)} \\
         &= \frac{cw}{0.100 \times 10 \times 1024}
          = \frac{cw}{1024}                        &&\text{(MBps)} \\
         &= \frac{cw}{8 \times 1024}               &&\text{(Mbps)}
\end{align*}

Since the congestion window is not allowed to grow to the bandwidth delay
product as explained above, we expect the real throughput $t(x)$ to be
approximately limited by $t'(cw_x)$. It is pretty clear from the above that
$t'(cw) = 0.8 \times c(x)$, and so in this particular setup, we would expect to
see the throughput follow the congestion window closely at these particular
scales because $t(x) \approx 0.8 \times c(x)$.

\subsection{What if pigs could fly?} % Optimal solution

\section{Conclusion}

%Subfig example
%\begin{figure}[h]
% \centering
% \subfloat[][cross-channel interference] {\
%   \scalebox{0.55}{\input{graphs/mp-int-2.4-ni.tex}}\label{graph:cc-interference}
% }
% \subfloat[][same-channel interference] {\
%   \scalebox{0.55}{\input{graphs/mp-int-2.4-i.tex}}\label{graph:sc-interference}
% }
%
% \caption{Interference experiments}\label{graph:interference}
%\end{figure}

\end{document}
% vim:textwidth=80:colorcolumn=80:
